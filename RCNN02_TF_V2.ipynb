{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCNN02_TF_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQN33EgH3LVX3HqbrUH5N4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-r-tanha/Deep-Learning-RCNN-Anomaly-detection/blob/master/RCNN02_TF_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "JxWYMygcDNMR",
        "outputId": "44674084-c33a-4213-d7aa-051621a462d9"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "from sklearn import preprocessing\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.preprocessing import scale, normalize, minmax_scale\r\n",
        "#from tensorflow.contrib import rnn\r\n",
        "#from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, LSTM, Dropout, RNN, GRU\r\n",
        "import numpy as np\r\n",
        "from google.colab import files\r\n",
        "import io\r\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\r\n",
        "from keras.utils import np_utils\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "#tbc=TensorBoardColab()\r\n",
        "\r\n",
        "uploaded = files.upload()\r\n",
        "\r\n",
        "for fn in uploaded.keys():\r\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\r\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://7fb61dce3318.ngrok.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f7329cb-cc10-42b5-9a5e-aef99903b6fb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f7329cb-cc10-42b5-9a5e-aef99903b6fb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 1000_Sample3.xlsx to 1000_Sample3.xlsx\n",
            "User uploaded file \"1000_Sample3.xlsx\" with length 452161 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktKZApdsR8xX"
      },
      "source": [
        "from keras.utils import np_utils\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.utils import plot_model\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers.recurrent import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRdQXE5dJb7F"
      },
      "source": [
        "writer = pd.ExcelWriter('Yexcel.xlsx')\r\n",
        "data_path=\"1000_Sample3.xlsx\"\r\n",
        "data=pd.read_excel(data_path,sheet_name='Sheet3')\r\n",
        "data=data.fillna(0.0)\r\n",
        "data.rename(index=data.Cell,inplace=True)\r\n",
        "data.drop('Cell',axis=1,inplace=True)\r\n",
        "data.Lable_target=data.Lable_target.replace(to_replace=['N','DS','SI','GD','SD','GI'],value=[1,2,3,4,5,6])\r\n",
        "X=data.drop('Lable_target',axis=1).values\r\n",
        "Y=data.Lable_target\r\n",
        "#print(Y)\r\n",
        "#Y.describe()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhFPB0EvJgBn"
      },
      "source": [
        "# creating a noise with the same dimension as the dataset \r\n",
        "mu, sigma = .75, 0.1 \r\n",
        "noise = np.random.normal(mu, sigma, [1052,31]) \r\n",
        "noise.shape\r\n",
        "#-------------\r\n",
        "\r\n",
        "wn = np.random.randn(len(X),31)\r\n",
        "data_wn=X+wn\r\n",
        "\r\n",
        "X=np.append (X,data_wn,axis=0)\r\n",
        "Y=np.append(Y,Y,axis=0)\r\n",
        "#print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnG2gmPcJk9y"
      },
      "source": [
        "Normalized_X=scale(X, axis=1)\r\n",
        "Mi=Normalized_X.min()   \r\n",
        "Normalized_X=(-1*Mi)+Normalized_X\r\n",
        "X_train,X_test,Y_train, Y_test = train_test_split(Normalized_X,Y,test_size=.2,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU70L47IJo0V",
        "outputId": "96cff7ab-4b24-45a3-90be-cde830caaa5b"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\r\n",
        "import keras\r\n",
        "tf.disable_v2_behavior() \r\n",
        "tf.reset_default_graph()\r\n",
        "\r\n",
        "Y_train = np_utils.to_categorical(Y_train)\r\n",
        "Y_test = np_utils.to_categorical(Y_test)\r\n",
        "\r\n",
        "Y_train = np.delete(Y_train , 0, 1)\r\n",
        "Y_test = np.delete(Y_test , 0, 1)\r\n",
        "# Training Parametersznum_steps = 30\r\n",
        "num_steps=500\r\n",
        "batch_size = 120\r\n",
        "display_step = 1\r\n",
        "strides = 1\r\n",
        "k = 1\r\n",
        "\r\n",
        "# Network Parameters\r\n",
        "num_input = 31  #  data input (img shape: 28*28)\r\n",
        "num_hidden = 100\r\n",
        "num_classes = 6  #  total classes (0-9 digits)\r\n",
        "dropout = 0.7  # Dropout, probability to keep units\r\n",
        "\r\n",
        "# tf Graph input\r\n",
        "X = tf.placeholder(tf.float32, [None, num_input])\r\n",
        "Y = tf.placeholder(tf.float32, [None, num_classes])\r\n",
        "keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)\r\n",
        "\r\n",
        "is_training = tf.placeholder(tf.bool, name='MODE')\r\n",
        "\r\n",
        "\r\n",
        "# Store layers weight & bias\r\n",
        "# The first three convolutional layer\r\n",
        "w_c_1 = tf.Variable(tf.random_normal([1, 3, 1, 28]))\r\n",
        "w_c_2 = tf.Variable(tf.random_normal([1, 3, 28, 56]))\r\n",
        "w_c_3 = tf.Variable(tf.random_normal([1, 3, 56, 112]))\r\n",
        "b_c_1 = tf.Variable(tf.zeros([28]))\r\n",
        "b_c_2 = tf.Variable(tf.zeros([56]))\r\n",
        "b_c_3 = tf.Variable(tf.zeros([112]))\r\n",
        "\r\n",
        "# The second three convolutional layer weights\r\n",
        "w_c_4 = tf.Variable(tf.random_normal([1, 3, 112, 224]))\r\n",
        "w_c_5 = tf.Variable(tf.random_normal([1, 3, 224, 448]))\r\n",
        "w_c_6 = tf.Variable(tf.random_normal([1, 3, 448, 896]))\r\n",
        "b_c_4 = tf.Variable(tf.zeros([224]))\r\n",
        "b_c_5 = tf.Variable(tf.zeros([448]))\r\n",
        "b_c_6 = tf.Variable(tf.zeros([896]))\r\n",
        "\r\n",
        "# Fully connected weight\r\n",
        "w_f_1 = tf.Variable(tf.random_normal([1 * 31 * 896, 1792])) # fully connected, 1*3*896 inputs, 2048 outputs\r\n",
        "w_f_2 = tf.Variable(tf.random_normal([1792, 896]))\r\n",
        "w_f_3 = tf.Variable(tf.random_normal([896, 448]))\r\n",
        "b_f_1 = tf.Variable(tf.zeros([1792]))\r\n",
        "b_f_2 = tf.Variable(tf.zeros([896]))\r\n",
        "b_f_3 = tf.Variable(tf.zeros([448]))\r\n",
        "\r\n",
        "# output layer weight\r\n",
        "w_out = tf.Variable(tf.random_normal([448, num_classes]))\r\n",
        "b_out = tf.Variable(tf.zeros([num_classes]))\r\n",
        "\r\n",
        "#\r\n",
        "# Define model\r\n",
        "x = tf.reshape(X, shape=[-1, 1, 31, 1])\r\n",
        "\r\n",
        "\r\n",
        "# first layer convolution\r\n",
        "conv1 = tf.nn.conv2d(x, w_c_1, strides=[1, 1, 1, 1], padding='SAME') + b_c_1\r\n",
        "conv1 = tf.compat.v1.layers.batch_normalization(conv1, training=True, center=True, scale=False)\r\n",
        "#conv1 = BatchNormalization(conv1, center=True, scale=False)\r\n",
        "conv1 = tf.nn.tanh(conv1)\r\n",
        "\r\n",
        "\r\n",
        "# second layer convolution\r\n",
        "conv2 = tf.nn.conv2d(conv1, w_c_2, strides=[1, strides, strides, 1], padding='SAME') + b_c_2\r\n",
        "conv2 = tf.compat.v1.layers.batch_normalization(conv2, training=True, center=True, scale=False)\r\n",
        "conv2 = tf.nn.tanh(conv2)\r\n",
        "conv2 = tf.nn.dropout(conv2, dropout)\r\n",
        "\r\n",
        "# third layer convolution\r\n",
        "conv3 = tf.nn.conv2d(conv2, w_c_3, strides=[1, strides, strides, 1], padding='SAME') + b_c_3\r\n",
        "conv3 = tf.compat.v1.layers.batch_normalization(conv3, training=True, center=True, scale=False)\r\n",
        "conv3 = tf.nn.tanh(conv3)\r\n",
        "\r\n",
        "# first Max Pooling (down-sampling)\r\n",
        "pool_1 = tf.nn.max_pool(conv3, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\r\n",
        "\r\n",
        "# fourth layer convolution\r\n",
        "conv4 = tf.nn.conv2d(pool_1, w_c_4, strides=[1, strides, strides, 1], padding='SAME') + b_c_4\r\n",
        "conv4 = tf.compat.v1.layers.batch_normalization(conv4, training=True, center=True, scale=False)\r\n",
        "conv4 = tf.nn.tanh(conv4)\r\n",
        "conv4 = tf.nn.dropout(conv4, dropout)\r\n",
        "\r\n",
        "# fifth layer convolution\r\n",
        "conv5 = tf.nn.conv2d(conv4, w_c_5, strides=[1, strides, strides, 1], padding='SAME') + b_c_5\r\n",
        "conv5 = tf.compat.v1.layers.batch_normalization(conv5, training=True, center=True, scale=False)\r\n",
        "conv5 = tf.nn.tanh(conv5)\r\n",
        "\r\n",
        "# sixth layer convolution\r\n",
        "conv6 = tf.nn.conv2d(conv5, w_c_6, strides=[1, strides, strides, 1], padding='SAME') + b_c_6\r\n",
        "conv6 = tf.compat.v1.layers.batch_normalization(conv6, training=True, center=True, scale=False)\r\n",
        "conv6 = tf.nn.tanh(conv6)\r\n",
        "conv6 = tf.nn.dropout(conv6, dropout)\r\n",
        "\r\n",
        "# second Max Pooling (down-sampling)\r\n",
        "pool_2 = tf.nn.max_pool(conv6, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\r\n",
        "\r\n",
        "pool_2 = tf.reshape(pool_2, [-1,31,896])\r\n",
        "\r\n",
        "# Define weights\r\n",
        "weights = {\r\n",
        "    'out': tf.Variable(tf.random_normal([num_hidden,num_classes]))\r\n",
        "}\r\n",
        "biases = {\r\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\r\n",
        "}\r\n",
        "\r\n",
        "def RNN(x, weights, biases):\r\n",
        "  \r\n",
        "    lstm_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_hidden,reuse=tf.AUTO_REUSE)\r\n",
        "    outputs, states = tf.compat.v1.nn.dynamic_rnn(lstm_cell, x , dtype=tf.float32)\r\n",
        "    return tf.matmul(outputs[:,-1], weights['out']) + biases['out']\r\n",
        "\r\n",
        "logits = RNN(pool_2, weights, biases)\r\n",
        "prediction = tf.nn.softmax(logits)\r\n",
        "\r\n",
        "#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$4\r\n",
        "# Define loss and optimizer\r\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\r\n",
        "\r\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer()\r\n",
        "train_op = optimizer.minimize(loss_op)\r\n",
        "\r\n",
        "# Evaluate model\r\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\r\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n",
        "\r\n",
        "# Initialize the variables (i.e. assign their default value)\r\n",
        "init = tf.global_variables_initializer()\r\n",
        "\r\n",
        "# Please don't change these.\r\n",
        "config = tf.ConfigProto()\r\n",
        "config.gpu_options.allow_growth = True\r\n",
        "\r\n",
        "#writer.close()\r\n",
        "A=pd.DataFrame({'loss':[], 'Acc':[]})\r\n",
        "\r\n",
        "# Start training\r\n",
        "with tf.Session(config=config) as sess:\r\n",
        "    # Run the initializer\r\n",
        "    sess.run(init)\r\n",
        "\r\n",
        "    for step in range(1, num_steps + 1):\r\n",
        "\r\n",
        "        sess.run(train_op, feed_dict={X: X_train, Y: Y_train, keep_prob: 0.7})\r\n",
        "        if step % display_step == 0 or step == 1:\r\n",
        "            # Calculate batch loss and accuracy\r\n",
        "            loss, acc= sess.run([loss_op, accuracy], feed_dict={X: X_train,\r\n",
        "                                                                 Y: Y_train,\r\n",
        "                                                                 keep_prob: 1.0})\r\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\r\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\r\n",
        "                  \"{:.3f}\".format(acc) )\r\n",
        "            df=pd.DataFrame({'loss':[loss], 'Acc':[acc]})\r\n",
        "            A=A.append(df) \r\n",
        "    saver=tf.train.Saver()\r\n",
        "    save_path=saver.save(sess,\"./TF_ModelwBN_for_Paper4.ckpt\")\r\n",
        "    #tbc.save_value(\"graph_name\", \"line_name\", num_steps, loss)\r\n",
        "    #tbc.flush_line(line_name)\r\n",
        "    #tbc.close()\r\n",
        "    print(\"Optimization Finished!\")    \r\n",
        "    print(\"Testing Accuracy:\", \\\r\n",
        "          sess.run(accuracy, feed_dict={X: X_test , Y: Y_test}))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/legacy_tf_layers/normalization.py:308: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  '`tf.layers.batch_normalization` is deprecated and '\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:702: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 2.6017, Training Accuracy= 0.466\n",
            "Step 2, Minibatch Loss= 1.8667, Training Accuracy= 0.462\n",
            "Step 3, Minibatch Loss= 1.6949, Training Accuracy= 0.495\n",
            "Step 4, Minibatch Loss= 1.6860, Training Accuracy= 0.504\n",
            "Step 5, Minibatch Loss= 1.4150, Training Accuracy= 0.529\n",
            "Step 6, Minibatch Loss= 1.4023, Training Accuracy= 0.533\n",
            "Step 7, Minibatch Loss= 1.3587, Training Accuracy= 0.541\n",
            "Step 8, Minibatch Loss= 1.3395, Training Accuracy= 0.541\n",
            "Step 9, Minibatch Loss= 1.2171, Training Accuracy= 0.562\n",
            "Step 10, Minibatch Loss= 1.1864, Training Accuracy= 0.562\n",
            "Step 11, Minibatch Loss= 1.1346, Training Accuracy= 0.570\n",
            "Step 12, Minibatch Loss= 1.1247, Training Accuracy= 0.557\n",
            "Step 13, Minibatch Loss= 1.1213, Training Accuracy= 0.563\n",
            "Step 14, Minibatch Loss= 1.0694, Training Accuracy= 0.565\n",
            "Step 15, Minibatch Loss= 1.0607, Training Accuracy= 0.556\n",
            "Step 16, Minibatch Loss= 0.9972, Training Accuracy= 0.589\n",
            "Step 17, Minibatch Loss= 1.0058, Training Accuracy= 0.578\n",
            "Step 18, Minibatch Loss= 0.9832, Training Accuracy= 0.589\n",
            "Step 19, Minibatch Loss= 0.9969, Training Accuracy= 0.595\n",
            "Step 20, Minibatch Loss= 0.9565, Training Accuracy= 0.619\n",
            "Step 21, Minibatch Loss= 0.9848, Training Accuracy= 0.601\n",
            "Step 22, Minibatch Loss= 0.9638, Training Accuracy= 0.614\n",
            "Step 23, Minibatch Loss= 0.9433, Training Accuracy= 0.601\n",
            "Step 24, Minibatch Loss= 0.8959, Training Accuracy= 0.638\n",
            "Step 25, Minibatch Loss= 0.9265, Training Accuracy= 0.618\n",
            "Step 26, Minibatch Loss= 0.8980, Training Accuracy= 0.625\n",
            "Step 27, Minibatch Loss= 0.8902, Training Accuracy= 0.618\n",
            "Step 28, Minibatch Loss= 0.9033, Training Accuracy= 0.628\n",
            "Step 29, Minibatch Loss= 0.8946, Training Accuracy= 0.638\n",
            "Step 30, Minibatch Loss= 0.8984, Training Accuracy= 0.639\n",
            "Step 31, Minibatch Loss= 0.9016, Training Accuracy= 0.626\n",
            "Step 32, Minibatch Loss= 0.8576, Training Accuracy= 0.653\n",
            "Step 33, Minibatch Loss= 0.8572, Training Accuracy= 0.642\n",
            "Step 34, Minibatch Loss= 0.8537, Training Accuracy= 0.660\n",
            "Step 35, Minibatch Loss= 0.8328, Training Accuracy= 0.663\n",
            "Step 36, Minibatch Loss= 0.8436, Training Accuracy= 0.654\n",
            "Step 37, Minibatch Loss= 0.8514, Training Accuracy= 0.649\n",
            "Step 38, Minibatch Loss= 0.8434, Training Accuracy= 0.656\n",
            "Step 39, Minibatch Loss= 0.8377, Training Accuracy= 0.649\n",
            "Step 40, Minibatch Loss= 0.8076, Training Accuracy= 0.665\n",
            "Step 41, Minibatch Loss= 0.8129, Training Accuracy= 0.657\n",
            "Step 42, Minibatch Loss= 0.8087, Training Accuracy= 0.669\n",
            "Step 43, Minibatch Loss= 0.8056, Training Accuracy= 0.668\n",
            "Step 44, Minibatch Loss= 0.7980, Training Accuracy= 0.674\n",
            "Step 45, Minibatch Loss= 0.7862, Training Accuracy= 0.674\n",
            "Step 46, Minibatch Loss= 0.8163, Training Accuracy= 0.671\n",
            "Step 47, Minibatch Loss= 0.7865, Training Accuracy= 0.679\n",
            "Step 48, Minibatch Loss= 0.7998, Training Accuracy= 0.670\n",
            "Step 49, Minibatch Loss= 0.7934, Training Accuracy= 0.674\n",
            "Step 50, Minibatch Loss= 0.7770, Training Accuracy= 0.679\n",
            "Step 51, Minibatch Loss= 0.7868, Training Accuracy= 0.693\n",
            "Step 52, Minibatch Loss= 0.7642, Training Accuracy= 0.694\n",
            "Step 53, Minibatch Loss= 0.7568, Training Accuracy= 0.699\n",
            "Step 54, Minibatch Loss= 0.7625, Training Accuracy= 0.686\n",
            "Step 55, Minibatch Loss= 0.7614, Training Accuracy= 0.689\n",
            "Step 56, Minibatch Loss= 0.7666, Training Accuracy= 0.691\n",
            "Step 57, Minibatch Loss= 0.7545, Training Accuracy= 0.693\n",
            "Step 58, Minibatch Loss= 0.7387, Training Accuracy= 0.710\n",
            "Step 59, Minibatch Loss= 0.7497, Training Accuracy= 0.696\n",
            "Step 60, Minibatch Loss= 0.7481, Training Accuracy= 0.704\n",
            "Step 61, Minibatch Loss= 0.7308, Training Accuracy= 0.692\n",
            "Step 62, Minibatch Loss= 0.7425, Training Accuracy= 0.706\n",
            "Step 63, Minibatch Loss= 0.7162, Training Accuracy= 0.711\n",
            "Step 64, Minibatch Loss= 0.7076, Training Accuracy= 0.709\n",
            "Step 65, Minibatch Loss= 0.7254, Training Accuracy= 0.701\n",
            "Step 66, Minibatch Loss= 0.7115, Training Accuracy= 0.716\n",
            "Step 67, Minibatch Loss= 0.7080, Training Accuracy= 0.703\n",
            "Step 68, Minibatch Loss= 0.7145, Training Accuracy= 0.711\n",
            "Step 69, Minibatch Loss= 0.7301, Training Accuracy= 0.706\n",
            "Step 70, Minibatch Loss= 0.6983, Training Accuracy= 0.712\n",
            "Step 71, Minibatch Loss= 0.7152, Training Accuracy= 0.702\n",
            "Step 72, Minibatch Loss= 0.6983, Training Accuracy= 0.716\n",
            "Step 73, Minibatch Loss= 0.6909, Training Accuracy= 0.719\n",
            "Step 74, Minibatch Loss= 0.6938, Training Accuracy= 0.712\n",
            "Step 75, Minibatch Loss= 0.6726, Training Accuracy= 0.728\n",
            "Step 76, Minibatch Loss= 0.6749, Training Accuracy= 0.727\n",
            "Step 77, Minibatch Loss= 0.6846, Training Accuracy= 0.717\n",
            "Step 78, Minibatch Loss= 0.6851, Training Accuracy= 0.724\n",
            "Step 79, Minibatch Loss= 0.6806, Training Accuracy= 0.720\n",
            "Step 80, Minibatch Loss= 0.6800, Training Accuracy= 0.718\n",
            "Step 81, Minibatch Loss= 0.6753, Training Accuracy= 0.723\n",
            "Step 82, Minibatch Loss= 0.6688, Training Accuracy= 0.738\n",
            "Step 83, Minibatch Loss= 0.6633, Training Accuracy= 0.725\n",
            "Step 84, Minibatch Loss= 0.6748, Training Accuracy= 0.730\n",
            "Step 85, Minibatch Loss= 0.6360, Training Accuracy= 0.751\n",
            "Step 86, Minibatch Loss= 0.6578, Training Accuracy= 0.747\n",
            "Step 87, Minibatch Loss= 0.6679, Training Accuracy= 0.728\n",
            "Step 88, Minibatch Loss= 0.6534, Training Accuracy= 0.739\n",
            "Step 89, Minibatch Loss= 0.6751, Training Accuracy= 0.722\n",
            "Step 90, Minibatch Loss= 0.6441, Training Accuracy= 0.742\n",
            "Step 91, Minibatch Loss= 0.6383, Training Accuracy= 0.737\n",
            "Step 92, Minibatch Loss= 0.6578, Training Accuracy= 0.733\n",
            "Step 93, Minibatch Loss= 0.6305, Training Accuracy= 0.740\n",
            "Step 94, Minibatch Loss= 0.6453, Training Accuracy= 0.737\n",
            "Step 95, Minibatch Loss= 0.6045, Training Accuracy= 0.752\n",
            "Step 96, Minibatch Loss= 0.6350, Training Accuracy= 0.755\n",
            "Step 97, Minibatch Loss= 0.6219, Training Accuracy= 0.738\n",
            "Step 98, Minibatch Loss= 0.6099, Training Accuracy= 0.755\n",
            "Step 99, Minibatch Loss= 0.6390, Training Accuracy= 0.741\n",
            "Step 100, Minibatch Loss= 0.6071, Training Accuracy= 0.764\n",
            "Step 101, Minibatch Loss= 0.6268, Training Accuracy= 0.755\n",
            "Step 102, Minibatch Loss= 0.6083, Training Accuracy= 0.751\n",
            "Step 103, Minibatch Loss= 0.6077, Training Accuracy= 0.763\n",
            "Step 104, Minibatch Loss= 0.6229, Training Accuracy= 0.743\n",
            "Step 105, Minibatch Loss= 0.6141, Training Accuracy= 0.745\n",
            "Step 106, Minibatch Loss= 0.6177, Training Accuracy= 0.742\n",
            "Step 107, Minibatch Loss= 0.5874, Training Accuracy= 0.756\n",
            "Step 108, Minibatch Loss= 0.5974, Training Accuracy= 0.766\n",
            "Step 109, Minibatch Loss= 0.5945, Training Accuracy= 0.759\n",
            "Step 110, Minibatch Loss= 0.5849, Training Accuracy= 0.765\n",
            "Step 111, Minibatch Loss= 0.6106, Training Accuracy= 0.756\n",
            "Step 112, Minibatch Loss= 0.5743, Training Accuracy= 0.758\n",
            "Step 113, Minibatch Loss= 0.5850, Training Accuracy= 0.761\n",
            "Step 114, Minibatch Loss= 0.5729, Training Accuracy= 0.763\n",
            "Step 115, Minibatch Loss= 0.5675, Training Accuracy= 0.781\n",
            "Step 116, Minibatch Loss= 0.5959, Training Accuracy= 0.762\n",
            "Step 117, Minibatch Loss= 0.5692, Training Accuracy= 0.769\n",
            "Step 118, Minibatch Loss= 0.5894, Training Accuracy= 0.748\n",
            "Step 119, Minibatch Loss= 0.5765, Training Accuracy= 0.761\n",
            "Step 120, Minibatch Loss= 0.5394, Training Accuracy= 0.791\n",
            "Step 121, Minibatch Loss= 0.5452, Training Accuracy= 0.783\n",
            "Step 122, Minibatch Loss= 0.5594, Training Accuracy= 0.771\n",
            "Step 123, Minibatch Loss= 0.5575, Training Accuracy= 0.780\n",
            "Step 124, Minibatch Loss= 0.5645, Training Accuracy= 0.775\n",
            "Step 125, Minibatch Loss= 0.5516, Training Accuracy= 0.783\n",
            "Step 126, Minibatch Loss= 0.5373, Training Accuracy= 0.778\n",
            "Step 127, Minibatch Loss= 0.5439, Training Accuracy= 0.779\n",
            "Step 128, Minibatch Loss= 0.5623, Training Accuracy= 0.773\n",
            "Step 129, Minibatch Loss= 0.5339, Training Accuracy= 0.775\n",
            "Step 130, Minibatch Loss= 0.5402, Training Accuracy= 0.771\n",
            "Step 131, Minibatch Loss= 0.5412, Training Accuracy= 0.783\n",
            "Step 132, Minibatch Loss= 0.5236, Training Accuracy= 0.791\n",
            "Step 133, Minibatch Loss= 0.5292, Training Accuracy= 0.787\n",
            "Step 134, Minibatch Loss= 0.5098, Training Accuracy= 0.786\n",
            "Step 135, Minibatch Loss= 0.5274, Training Accuracy= 0.779\n",
            "Step 136, Minibatch Loss= 0.5255, Training Accuracy= 0.778\n",
            "Step 137, Minibatch Loss= 0.5152, Training Accuracy= 0.796\n",
            "Step 138, Minibatch Loss= 0.5048, Training Accuracy= 0.797\n",
            "Step 139, Minibatch Loss= 0.5138, Training Accuracy= 0.799\n",
            "Step 140, Minibatch Loss= 0.5145, Training Accuracy= 0.788\n",
            "Step 141, Minibatch Loss= 0.4972, Training Accuracy= 0.794\n",
            "Step 142, Minibatch Loss= 0.5118, Training Accuracy= 0.791\n",
            "Step 143, Minibatch Loss= 0.5019, Training Accuracy= 0.791\n",
            "Step 144, Minibatch Loss= 0.4903, Training Accuracy= 0.799\n",
            "Step 145, Minibatch Loss= 0.5093, Training Accuracy= 0.796\n",
            "Step 146, Minibatch Loss= 0.4951, Training Accuracy= 0.800\n",
            "Step 147, Minibatch Loss= 0.5126, Training Accuracy= 0.800\n",
            "Step 148, Minibatch Loss= 0.5168, Training Accuracy= 0.805\n",
            "Step 149, Minibatch Loss= 0.4914, Training Accuracy= 0.803\n",
            "Step 150, Minibatch Loss= 0.4884, Training Accuracy= 0.805\n",
            "Step 151, Minibatch Loss= 0.4811, Training Accuracy= 0.813\n",
            "Step 152, Minibatch Loss= 0.4803, Training Accuracy= 0.815\n",
            "Step 153, Minibatch Loss= 0.4796, Training Accuracy= 0.808\n",
            "Step 154, Minibatch Loss= 0.4779, Training Accuracy= 0.805\n",
            "Step 155, Minibatch Loss= 0.4919, Training Accuracy= 0.799\n",
            "Step 156, Minibatch Loss= 0.4851, Training Accuracy= 0.805\n",
            "Step 157, Minibatch Loss= 0.4785, Training Accuracy= 0.806\n",
            "Step 158, Minibatch Loss= 0.4774, Training Accuracy= 0.805\n",
            "Step 159, Minibatch Loss= 0.4810, Training Accuracy= 0.807\n",
            "Step 160, Minibatch Loss= 0.4572, Training Accuracy= 0.811\n",
            "Step 161, Minibatch Loss= 0.4518, Training Accuracy= 0.825\n",
            "Step 162, Minibatch Loss= 0.4640, Training Accuracy= 0.815\n",
            "Step 163, Minibatch Loss= 0.4549, Training Accuracy= 0.827\n",
            "Step 164, Minibatch Loss= 0.4514, Training Accuracy= 0.827\n",
            "Step 165, Minibatch Loss= 0.4398, Training Accuracy= 0.828\n",
            "Step 166, Minibatch Loss= 0.4451, Training Accuracy= 0.819\n",
            "Step 167, Minibatch Loss= 0.4256, Training Accuracy= 0.828\n",
            "Step 168, Minibatch Loss= 0.4673, Training Accuracy= 0.809\n",
            "Step 169, Minibatch Loss= 0.4304, Training Accuracy= 0.824\n",
            "Step 170, Minibatch Loss= 0.4465, Training Accuracy= 0.822\n",
            "Step 171, Minibatch Loss= 0.4601, Training Accuracy= 0.809\n",
            "Step 172, Minibatch Loss= 0.4356, Training Accuracy= 0.830\n",
            "Step 173, Minibatch Loss= 0.4515, Training Accuracy= 0.815\n",
            "Step 174, Minibatch Loss= 0.4391, Training Accuracy= 0.827\n",
            "Step 175, Minibatch Loss= 0.4438, Training Accuracy= 0.827\n",
            "Step 176, Minibatch Loss= 0.4257, Training Accuracy= 0.828\n",
            "Step 177, Minibatch Loss= 0.4179, Training Accuracy= 0.832\n",
            "Step 178, Minibatch Loss= 0.4171, Training Accuracy= 0.827\n",
            "Step 179, Minibatch Loss= 0.4265, Training Accuracy= 0.824\n",
            "Step 180, Minibatch Loss= 0.4258, Training Accuracy= 0.824\n",
            "Step 181, Minibatch Loss= 0.3908, Training Accuracy= 0.844\n",
            "Step 182, Minibatch Loss= 0.4150, Training Accuracy= 0.821\n",
            "Step 183, Minibatch Loss= 0.4208, Training Accuracy= 0.835\n",
            "Step 184, Minibatch Loss= 0.4163, Training Accuracy= 0.837\n",
            "Step 185, Minibatch Loss= 0.4028, Training Accuracy= 0.841\n",
            "Step 186, Minibatch Loss= 0.4079, Training Accuracy= 0.837\n",
            "Step 187, Minibatch Loss= 0.4171, Training Accuracy= 0.837\n",
            "Step 188, Minibatch Loss= 0.4075, Training Accuracy= 0.836\n",
            "Step 189, Minibatch Loss= 0.4189, Training Accuracy= 0.828\n",
            "Step 190, Minibatch Loss= 0.3933, Training Accuracy= 0.844\n",
            "Step 191, Minibatch Loss= 0.3923, Training Accuracy= 0.840\n",
            "Step 192, Minibatch Loss= 0.3986, Training Accuracy= 0.841\n",
            "Step 193, Minibatch Loss= 0.3835, Training Accuracy= 0.843\n",
            "Step 194, Minibatch Loss= 0.3920, Training Accuracy= 0.853\n",
            "Step 195, Minibatch Loss= 0.3995, Training Accuracy= 0.845\n",
            "Step 196, Minibatch Loss= 0.3914, Training Accuracy= 0.843\n",
            "Step 197, Minibatch Loss= 0.3954, Training Accuracy= 0.848\n",
            "Step 198, Minibatch Loss= 0.3797, Training Accuracy= 0.846\n",
            "Step 199, Minibatch Loss= 0.3771, Training Accuracy= 0.854\n",
            "Step 200, Minibatch Loss= 0.3815, Training Accuracy= 0.848\n",
            "Step 201, Minibatch Loss= 0.3733, Training Accuracy= 0.847\n",
            "Step 202, Minibatch Loss= 0.3684, Training Accuracy= 0.855\n",
            "Step 203, Minibatch Loss= 0.3778, Training Accuracy= 0.851\n",
            "Step 204, Minibatch Loss= 0.3761, Training Accuracy= 0.846\n",
            "Step 205, Minibatch Loss= 0.3707, Training Accuracy= 0.850\n",
            "Step 206, Minibatch Loss= 0.3648, Training Accuracy= 0.846\n",
            "Step 207, Minibatch Loss= 0.3808, Training Accuracy= 0.857\n",
            "Step 208, Minibatch Loss= 0.3652, Training Accuracy= 0.865\n",
            "Step 209, Minibatch Loss= 0.3517, Training Accuracy= 0.858\n",
            "Step 210, Minibatch Loss= 0.3540, Training Accuracy= 0.865\n",
            "Step 211, Minibatch Loss= 0.3403, Training Accuracy= 0.867\n",
            "Step 212, Minibatch Loss= 0.3362, Training Accuracy= 0.871\n",
            "Step 213, Minibatch Loss= 0.3494, Training Accuracy= 0.863\n",
            "Step 214, Minibatch Loss= 0.3539, Training Accuracy= 0.859\n",
            "Step 215, Minibatch Loss= 0.3396, Training Accuracy= 0.872\n",
            "Step 216, Minibatch Loss= 0.3402, Training Accuracy= 0.867\n",
            "Step 217, Minibatch Loss= 0.3391, Training Accuracy= 0.868\n",
            "Step 218, Minibatch Loss= 0.3366, Training Accuracy= 0.875\n",
            "Step 219, Minibatch Loss= 0.3234, Training Accuracy= 0.880\n",
            "Step 220, Minibatch Loss= 0.3358, Training Accuracy= 0.868\n",
            "Step 221, Minibatch Loss= 0.3257, Training Accuracy= 0.876\n",
            "Step 222, Minibatch Loss= 0.3180, Training Accuracy= 0.882\n",
            "Step 223, Minibatch Loss= 0.3182, Training Accuracy= 0.872\n",
            "Step 224, Minibatch Loss= 0.3187, Training Accuracy= 0.872\n",
            "Step 225, Minibatch Loss= 0.3095, Training Accuracy= 0.872\n",
            "Step 226, Minibatch Loss= 0.3212, Training Accuracy= 0.877\n",
            "Step 227, Minibatch Loss= 0.3216, Training Accuracy= 0.877\n",
            "Step 228, Minibatch Loss= 0.3041, Training Accuracy= 0.882\n",
            "Step 229, Minibatch Loss= 0.3062, Training Accuracy= 0.881\n",
            "Step 230, Minibatch Loss= 0.3340, Training Accuracy= 0.872\n",
            "Step 231, Minibatch Loss= 0.3141, Training Accuracy= 0.878\n",
            "Step 232, Minibatch Loss= 0.3004, Training Accuracy= 0.895\n",
            "Step 233, Minibatch Loss= 0.3100, Training Accuracy= 0.869\n",
            "Step 234, Minibatch Loss= 0.3168, Training Accuracy= 0.884\n",
            "Step 235, Minibatch Loss= 0.2970, Training Accuracy= 0.885\n",
            "Step 236, Minibatch Loss= 0.3116, Training Accuracy= 0.878\n",
            "Step 237, Minibatch Loss= 0.2958, Training Accuracy= 0.881\n",
            "Step 238, Minibatch Loss= 0.3117, Training Accuracy= 0.872\n",
            "Step 239, Minibatch Loss= 0.2974, Training Accuracy= 0.882\n",
            "Step 240, Minibatch Loss= 0.2880, Training Accuracy= 0.889\n",
            "Step 241, Minibatch Loss= 0.2998, Training Accuracy= 0.878\n",
            "Step 242, Minibatch Loss= 0.3015, Training Accuracy= 0.885\n",
            "Step 243, Minibatch Loss= 0.2874, Training Accuracy= 0.895\n",
            "Step 244, Minibatch Loss= 0.2881, Training Accuracy= 0.889\n",
            "Step 245, Minibatch Loss= 0.2880, Training Accuracy= 0.890\n",
            "Step 246, Minibatch Loss= 0.2731, Training Accuracy= 0.898\n",
            "Step 247, Minibatch Loss= 0.2991, Training Accuracy= 0.878\n",
            "Step 248, Minibatch Loss= 0.2735, Training Accuracy= 0.891\n",
            "Step 249, Minibatch Loss= 0.2842, Training Accuracy= 0.893\n",
            "Step 250, Minibatch Loss= 0.2809, Training Accuracy= 0.891\n",
            "Step 251, Minibatch Loss= 0.2613, Training Accuracy= 0.901\n",
            "Step 252, Minibatch Loss= 0.2787, Training Accuracy= 0.891\n",
            "Step 253, Minibatch Loss= 0.2639, Training Accuracy= 0.903\n",
            "Step 254, Minibatch Loss= 0.2572, Training Accuracy= 0.895\n",
            "Step 255, Minibatch Loss= 0.2687, Training Accuracy= 0.891\n",
            "Step 256, Minibatch Loss= 0.2792, Training Accuracy= 0.888\n",
            "Step 257, Minibatch Loss= 0.2624, Training Accuracy= 0.900\n",
            "Step 258, Minibatch Loss= 0.2611, Training Accuracy= 0.899\n",
            "Step 259, Minibatch Loss= 0.2662, Training Accuracy= 0.890\n",
            "Step 260, Minibatch Loss= 0.2570, Training Accuracy= 0.915\n",
            "Step 261, Minibatch Loss= 0.2494, Training Accuracy= 0.900\n",
            "Step 262, Minibatch Loss= 0.2581, Training Accuracy= 0.903\n",
            "Step 263, Minibatch Loss= 0.2551, Training Accuracy= 0.900\n",
            "Step 264, Minibatch Loss= 0.2396, Training Accuracy= 0.901\n",
            "Step 265, Minibatch Loss= 0.2359, Training Accuracy= 0.908\n",
            "Step 266, Minibatch Loss= 0.2352, Training Accuracy= 0.909\n",
            "Step 267, Minibatch Loss= 0.2659, Training Accuracy= 0.892\n",
            "Step 268, Minibatch Loss= 0.2562, Training Accuracy= 0.908\n",
            "Step 269, Minibatch Loss= 0.2493, Training Accuracy= 0.903\n",
            "Step 270, Minibatch Loss= 0.2450, Training Accuracy= 0.914\n",
            "Step 271, Minibatch Loss= 0.2415, Training Accuracy= 0.910\n",
            "Step 272, Minibatch Loss= 0.2225, Training Accuracy= 0.915\n",
            "Step 273, Minibatch Loss= 0.2253, Training Accuracy= 0.916\n",
            "Step 274, Minibatch Loss= 0.2374, Training Accuracy= 0.904\n",
            "Step 275, Minibatch Loss= 0.2160, Training Accuracy= 0.917\n",
            "Step 276, Minibatch Loss= 0.2342, Training Accuracy= 0.911\n",
            "Step 277, Minibatch Loss= 0.2197, Training Accuracy= 0.916\n",
            "Step 278, Minibatch Loss= 0.2251, Training Accuracy= 0.914\n",
            "Step 279, Minibatch Loss= 0.2204, Training Accuracy= 0.916\n",
            "Step 280, Minibatch Loss= 0.2099, Training Accuracy= 0.920\n",
            "Step 281, Minibatch Loss= 0.2208, Training Accuracy= 0.909\n",
            "Step 282, Minibatch Loss= 0.2100, Training Accuracy= 0.914\n",
            "Step 283, Minibatch Loss= 0.2308, Training Accuracy= 0.908\n",
            "Step 284, Minibatch Loss= 0.2399, Training Accuracy= 0.914\n",
            "Step 285, Minibatch Loss= 0.2096, Training Accuracy= 0.922\n",
            "Step 286, Minibatch Loss= 0.2082, Training Accuracy= 0.914\n",
            "Step 287, Minibatch Loss= 0.2013, Training Accuracy= 0.929\n",
            "Step 288, Minibatch Loss= 0.2063, Training Accuracy= 0.925\n",
            "Step 289, Minibatch Loss= 0.2134, Training Accuracy= 0.917\n",
            "Step 290, Minibatch Loss= 0.1913, Training Accuracy= 0.932\n",
            "Step 291, Minibatch Loss= 0.2129, Training Accuracy= 0.920\n",
            "Step 292, Minibatch Loss= 0.2020, Training Accuracy= 0.925\n",
            "Step 293, Minibatch Loss= 0.1894, Training Accuracy= 0.930\n",
            "Step 294, Minibatch Loss= 0.2017, Training Accuracy= 0.923\n",
            "Step 295, Minibatch Loss= 0.2050, Training Accuracy= 0.924\n",
            "Step 296, Minibatch Loss= 0.2135, Training Accuracy= 0.918\n",
            "Step 297, Minibatch Loss= 0.2005, Training Accuracy= 0.920\n",
            "Step 298, Minibatch Loss= 0.2020, Training Accuracy= 0.930\n",
            "Step 299, Minibatch Loss= 0.1968, Training Accuracy= 0.928\n",
            "Step 300, Minibatch Loss= 0.1802, Training Accuracy= 0.933\n",
            "Step 301, Minibatch Loss= 0.1947, Training Accuracy= 0.927\n",
            "Step 302, Minibatch Loss= 0.1834, Training Accuracy= 0.929\n",
            "Step 303, Minibatch Loss= 0.1992, Training Accuracy= 0.922\n",
            "Step 304, Minibatch Loss= 0.1761, Training Accuracy= 0.933\n",
            "Step 305, Minibatch Loss= 0.1966, Training Accuracy= 0.926\n",
            "Step 306, Minibatch Loss= 0.1800, Training Accuracy= 0.934\n",
            "Step 307, Minibatch Loss= 0.1707, Training Accuracy= 0.935\n",
            "Step 308, Minibatch Loss= 0.1789, Training Accuracy= 0.940\n",
            "Step 309, Minibatch Loss= 0.1936, Training Accuracy= 0.929\n",
            "Step 310, Minibatch Loss= 0.1872, Training Accuracy= 0.929\n",
            "Step 311, Minibatch Loss= 0.1978, Training Accuracy= 0.926\n",
            "Step 312, Minibatch Loss= 0.1941, Training Accuracy= 0.926\n",
            "Step 313, Minibatch Loss= 0.1733, Training Accuracy= 0.939\n",
            "Step 314, Minibatch Loss= 0.1791, Training Accuracy= 0.928\n",
            "Step 315, Minibatch Loss= 0.1693, Training Accuracy= 0.933\n",
            "Step 316, Minibatch Loss= 0.1716, Training Accuracy= 0.941\n",
            "Step 317, Minibatch Loss= 0.1808, Training Accuracy= 0.930\n",
            "Step 318, Minibatch Loss= 0.1713, Training Accuracy= 0.938\n",
            "Step 319, Minibatch Loss= 0.1709, Training Accuracy= 0.942\n",
            "Step 320, Minibatch Loss= 0.1693, Training Accuracy= 0.941\n",
            "Step 321, Minibatch Loss= 0.1628, Training Accuracy= 0.938\n",
            "Step 322, Minibatch Loss= 0.1583, Training Accuracy= 0.940\n",
            "Step 323, Minibatch Loss= 0.1702, Training Accuracy= 0.936\n",
            "Step 324, Minibatch Loss= 0.1648, Training Accuracy= 0.939\n",
            "Step 325, Minibatch Loss= 0.1630, Training Accuracy= 0.936\n",
            "Step 326, Minibatch Loss= 0.1631, Training Accuracy= 0.941\n",
            "Step 327, Minibatch Loss= 0.1600, Training Accuracy= 0.944\n",
            "Step 328, Minibatch Loss= 0.1573, Training Accuracy= 0.941\n",
            "Step 329, Minibatch Loss= 0.1682, Training Accuracy= 0.932\n",
            "Step 330, Minibatch Loss= 0.1513, Training Accuracy= 0.947\n",
            "Step 331, Minibatch Loss= 0.1622, Training Accuracy= 0.944\n",
            "Step 332, Minibatch Loss= 0.1579, Training Accuracy= 0.935\n",
            "Step 333, Minibatch Loss= 0.1567, Training Accuracy= 0.939\n",
            "Step 334, Minibatch Loss= 0.1358, Training Accuracy= 0.950\n",
            "Step 335, Minibatch Loss= 0.1554, Training Accuracy= 0.942\n",
            "Step 336, Minibatch Loss= 0.1412, Training Accuracy= 0.952\n",
            "Step 337, Minibatch Loss= 0.1613, Training Accuracy= 0.945\n",
            "Step 338, Minibatch Loss= 0.1460, Training Accuracy= 0.951\n",
            "Step 339, Minibatch Loss= 0.1466, Training Accuracy= 0.945\n",
            "Step 340, Minibatch Loss= 0.1425, Training Accuracy= 0.949\n",
            "Step 341, Minibatch Loss= 0.1501, Training Accuracy= 0.950\n",
            "Step 342, Minibatch Loss= 0.1412, Training Accuracy= 0.949\n",
            "Step 343, Minibatch Loss= 0.1283, Training Accuracy= 0.956\n",
            "Step 344, Minibatch Loss= 0.1543, Training Accuracy= 0.942\n",
            "Step 345, Minibatch Loss= 0.1359, Training Accuracy= 0.951\n",
            "Step 346, Minibatch Loss= 0.1450, Training Accuracy= 0.950\n",
            "Step 347, Minibatch Loss= 0.1263, Training Accuracy= 0.950\n",
            "Step 348, Minibatch Loss= 0.1331, Training Accuracy= 0.951\n",
            "Step 349, Minibatch Loss= 0.1457, Training Accuracy= 0.950\n",
            "Step 350, Minibatch Loss= 0.1342, Training Accuracy= 0.951\n",
            "Step 351, Minibatch Loss= 0.1466, Training Accuracy= 0.948\n",
            "Step 352, Minibatch Loss= 0.1285, Training Accuracy= 0.954\n",
            "Step 353, Minibatch Loss= 0.1249, Training Accuracy= 0.957\n",
            "Step 354, Minibatch Loss= 0.1405, Training Accuracy= 0.948\n",
            "Step 355, Minibatch Loss= 0.1207, Training Accuracy= 0.955\n",
            "Step 356, Minibatch Loss= 0.1330, Training Accuracy= 0.948\n",
            "Step 357, Minibatch Loss= 0.1167, Training Accuracy= 0.962\n",
            "Step 358, Minibatch Loss= 0.1237, Training Accuracy= 0.953\n",
            "Step 359, Minibatch Loss= 0.1388, Training Accuracy= 0.944\n",
            "Step 360, Minibatch Loss= 0.1216, Training Accuracy= 0.955\n",
            "Step 361, Minibatch Loss= 0.1307, Training Accuracy= 0.958\n",
            "Step 362, Minibatch Loss= 0.1340, Training Accuracy= 0.948\n",
            "Step 363, Minibatch Loss= 0.1214, Training Accuracy= 0.954\n",
            "Step 364, Minibatch Loss= 0.1125, Training Accuracy= 0.965\n",
            "Step 365, Minibatch Loss= 0.1275, Training Accuracy= 0.961\n",
            "Step 366, Minibatch Loss= 0.1143, Training Accuracy= 0.959\n",
            "Step 367, Minibatch Loss= 0.1111, Training Accuracy= 0.958\n",
            "Step 368, Minibatch Loss= 0.1060, Training Accuracy= 0.963\n",
            "Step 369, Minibatch Loss= 0.1241, Training Accuracy= 0.952\n",
            "Step 370, Minibatch Loss= 0.1257, Training Accuracy= 0.957\n",
            "Step 371, Minibatch Loss= 0.1152, Training Accuracy= 0.958\n",
            "Step 372, Minibatch Loss= 0.1100, Training Accuracy= 0.960\n",
            "Step 373, Minibatch Loss= 0.1201, Training Accuracy= 0.955\n",
            "Step 374, Minibatch Loss= 0.1173, Training Accuracy= 0.961\n",
            "Step 375, Minibatch Loss= 0.1157, Training Accuracy= 0.961\n",
            "Step 376, Minibatch Loss= 0.1078, Training Accuracy= 0.964\n",
            "Step 377, Minibatch Loss= 0.1159, Training Accuracy= 0.964\n",
            "Step 378, Minibatch Loss= 0.1036, Training Accuracy= 0.966\n",
            "Step 379, Minibatch Loss= 0.1074, Training Accuracy= 0.964\n",
            "Step 380, Minibatch Loss= 0.1010, Training Accuracy= 0.967\n",
            "Step 381, Minibatch Loss= 0.1063, Training Accuracy= 0.964\n",
            "Step 382, Minibatch Loss= 0.1215, Training Accuracy= 0.955\n",
            "Step 383, Minibatch Loss= 0.1085, Training Accuracy= 0.964\n",
            "Step 384, Minibatch Loss= 0.1025, Training Accuracy= 0.962\n",
            "Step 385, Minibatch Loss= 0.1111, Training Accuracy= 0.958\n",
            "Step 386, Minibatch Loss= 0.1091, Training Accuracy= 0.965\n",
            "Step 387, Minibatch Loss= 0.0996, Training Accuracy= 0.967\n",
            "Step 388, Minibatch Loss= 0.0909, Training Accuracy= 0.968\n",
            "Step 389, Minibatch Loss= 0.0874, Training Accuracy= 0.970\n",
            "Step 390, Minibatch Loss= 0.1148, Training Accuracy= 0.958\n",
            "Step 391, Minibatch Loss= 0.1029, Training Accuracy= 0.963\n",
            "Step 392, Minibatch Loss= 0.1010, Training Accuracy= 0.964\n",
            "Step 393, Minibatch Loss= 0.0856, Training Accuracy= 0.976\n",
            "Step 394, Minibatch Loss= 0.0937, Training Accuracy= 0.971\n",
            "Step 395, Minibatch Loss= 0.0988, Training Accuracy= 0.969\n",
            "Step 396, Minibatch Loss= 0.0926, Training Accuracy= 0.967\n",
            "Step 397, Minibatch Loss= 0.1034, Training Accuracy= 0.965\n",
            "Step 398, Minibatch Loss= 0.1064, Training Accuracy= 0.963\n",
            "Step 399, Minibatch Loss= 0.1075, Training Accuracy= 0.964\n",
            "Step 400, Minibatch Loss= 0.0976, Training Accuracy= 0.966\n",
            "Step 401, Minibatch Loss= 0.1021, Training Accuracy= 0.959\n",
            "Step 402, Minibatch Loss= 0.0872, Training Accuracy= 0.971\n",
            "Step 403, Minibatch Loss= 0.0999, Training Accuracy= 0.963\n",
            "Step 404, Minibatch Loss= 0.0941, Training Accuracy= 0.968\n",
            "Step 405, Minibatch Loss= 0.0933, Training Accuracy= 0.971\n",
            "Step 406, Minibatch Loss= 0.0853, Training Accuracy= 0.972\n",
            "Step 407, Minibatch Loss= 0.0945, Training Accuracy= 0.966\n",
            "Step 408, Minibatch Loss= 0.0918, Training Accuracy= 0.969\n",
            "Step 409, Minibatch Loss= 0.0786, Training Accuracy= 0.973\n",
            "Step 410, Minibatch Loss= 0.0892, Training Accuracy= 0.971\n",
            "Step 411, Minibatch Loss= 0.0868, Training Accuracy= 0.973\n",
            "Step 412, Minibatch Loss= 0.1001, Training Accuracy= 0.957\n",
            "Step 413, Minibatch Loss= 0.0905, Training Accuracy= 0.966\n",
            "Step 414, Minibatch Loss= 0.0812, Training Accuracy= 0.970\n",
            "Step 415, Minibatch Loss= 0.0894, Training Accuracy= 0.963\n",
            "Step 416, Minibatch Loss= 0.0819, Training Accuracy= 0.972\n",
            "Step 417, Minibatch Loss= 0.0929, Training Accuracy= 0.970\n",
            "Step 418, Minibatch Loss= 0.0760, Training Accuracy= 0.976\n",
            "Step 419, Minibatch Loss= 0.0868, Training Accuracy= 0.969\n",
            "Step 420, Minibatch Loss= 0.0816, Training Accuracy= 0.973\n",
            "Step 421, Minibatch Loss= 0.0831, Training Accuracy= 0.972\n",
            "Step 422, Minibatch Loss= 0.0700, Training Accuracy= 0.976\n",
            "Step 423, Minibatch Loss= 0.0924, Training Accuracy= 0.967\n",
            "Step 424, Minibatch Loss= 0.0768, Training Accuracy= 0.975\n",
            "Step 425, Minibatch Loss= 0.1032, Training Accuracy= 0.964\n",
            "Step 426, Minibatch Loss= 0.0793, Training Accuracy= 0.970\n",
            "Step 427, Minibatch Loss= 0.0814, Training Accuracy= 0.971\n",
            "Step 428, Minibatch Loss= 0.0834, Training Accuracy= 0.971\n",
            "Step 429, Minibatch Loss= 0.0798, Training Accuracy= 0.972\n",
            "Step 430, Minibatch Loss= 0.0718, Training Accuracy= 0.974\n",
            "Step 431, Minibatch Loss= 0.0763, Training Accuracy= 0.971\n",
            "Step 432, Minibatch Loss= 0.0881, Training Accuracy= 0.974\n",
            "Step 433, Minibatch Loss= 0.0741, Training Accuracy= 0.974\n",
            "Step 434, Minibatch Loss= 0.0713, Training Accuracy= 0.977\n",
            "Step 435, Minibatch Loss= 0.0732, Training Accuracy= 0.976\n",
            "Step 436, Minibatch Loss= 0.0890, Training Accuracy= 0.969\n",
            "Step 437, Minibatch Loss= 0.0756, Training Accuracy= 0.976\n",
            "Step 438, Minibatch Loss= 0.0692, Training Accuracy= 0.976\n",
            "Step 439, Minibatch Loss= 0.0655, Training Accuracy= 0.978\n",
            "Step 440, Minibatch Loss= 0.0838, Training Accuracy= 0.974\n",
            "Step 441, Minibatch Loss= 0.0798, Training Accuracy= 0.975\n",
            "Step 442, Minibatch Loss= 0.0733, Training Accuracy= 0.974\n",
            "Step 443, Minibatch Loss= 0.0806, Training Accuracy= 0.974\n",
            "Step 444, Minibatch Loss= 0.0667, Training Accuracy= 0.980\n",
            "Step 445, Minibatch Loss= 0.0811, Training Accuracy= 0.971\n",
            "Step 446, Minibatch Loss= 0.0761, Training Accuracy= 0.974\n",
            "Step 447, Minibatch Loss= 0.0718, Training Accuracy= 0.977\n",
            "Step 448, Minibatch Loss= 0.0792, Training Accuracy= 0.975\n",
            "Step 449, Minibatch Loss= 0.0772, Training Accuracy= 0.971\n",
            "Step 450, Minibatch Loss= 0.0767, Training Accuracy= 0.976\n",
            "Step 451, Minibatch Loss= 0.0635, Training Accuracy= 0.980\n",
            "Step 452, Minibatch Loss= 0.0818, Training Accuracy= 0.975\n",
            "Step 453, Minibatch Loss= 0.0631, Training Accuracy= 0.980\n",
            "Step 454, Minibatch Loss= 0.0740, Training Accuracy= 0.974\n",
            "Step 455, Minibatch Loss= 0.0685, Training Accuracy= 0.981\n",
            "Step 456, Minibatch Loss= 0.0731, Training Accuracy= 0.976\n",
            "Step 457, Minibatch Loss= 0.0779, Training Accuracy= 0.973\n",
            "Step 458, Minibatch Loss= 0.0764, Training Accuracy= 0.976\n",
            "Step 459, Minibatch Loss= 0.0627, Training Accuracy= 0.979\n",
            "Step 460, Minibatch Loss= 0.0574, Training Accuracy= 0.982\n",
            "Step 461, Minibatch Loss= 0.0612, Training Accuracy= 0.979\n",
            "Step 462, Minibatch Loss= 0.0725, Training Accuracy= 0.976\n",
            "Step 463, Minibatch Loss= 0.0704, Training Accuracy= 0.977\n",
            "Step 464, Minibatch Loss= 0.0624, Training Accuracy= 0.980\n",
            "Step 465, Minibatch Loss= 0.0699, Training Accuracy= 0.975\n",
            "Step 466, Minibatch Loss= 0.0742, Training Accuracy= 0.976\n",
            "Step 467, Minibatch Loss= 0.0720, Training Accuracy= 0.975\n",
            "Step 468, Minibatch Loss= 0.0611, Training Accuracy= 0.983\n",
            "Step 469, Minibatch Loss= 0.0604, Training Accuracy= 0.979\n",
            "Step 470, Minibatch Loss= 0.0712, Training Accuracy= 0.976\n",
            "Step 471, Minibatch Loss= 0.0590, Training Accuracy= 0.981\n",
            "Step 472, Minibatch Loss= 0.0563, Training Accuracy= 0.982\n",
            "Step 473, Minibatch Loss= 0.0622, Training Accuracy= 0.980\n",
            "Step 474, Minibatch Loss= 0.0597, Training Accuracy= 0.980\n",
            "Step 475, Minibatch Loss= 0.0540, Training Accuracy= 0.985\n",
            "Step 476, Minibatch Loss= 0.0533, Training Accuracy= 0.983\n",
            "Step 477, Minibatch Loss= 0.0563, Training Accuracy= 0.981\n",
            "Step 478, Minibatch Loss= 0.0516, Training Accuracy= 0.985\n",
            "Step 479, Minibatch Loss= 0.0642, Training Accuracy= 0.976\n",
            "Step 480, Minibatch Loss= 0.0704, Training Accuracy= 0.979\n",
            "Step 481, Minibatch Loss= 0.0579, Training Accuracy= 0.980\n",
            "Step 482, Minibatch Loss= 0.0534, Training Accuracy= 0.983\n",
            "Step 483, Minibatch Loss= 0.0589, Training Accuracy= 0.980\n",
            "Step 484, Minibatch Loss= 0.0601, Training Accuracy= 0.980\n",
            "Step 485, Minibatch Loss= 0.0538, Training Accuracy= 0.982\n",
            "Step 486, Minibatch Loss= 0.0588, Training Accuracy= 0.980\n",
            "Step 487, Minibatch Loss= 0.0470, Training Accuracy= 0.982\n",
            "Step 488, Minibatch Loss= 0.0582, Training Accuracy= 0.983\n",
            "Step 489, Minibatch Loss= 0.0644, Training Accuracy= 0.977\n",
            "Step 490, Minibatch Loss= 0.0602, Training Accuracy= 0.977\n",
            "Step 491, Minibatch Loss= 0.0524, Training Accuracy= 0.984\n",
            "Step 492, Minibatch Loss= 0.0610, Training Accuracy= 0.979\n",
            "Step 493, Minibatch Loss= 0.0607, Training Accuracy= 0.980\n",
            "Step 494, Minibatch Loss= 0.0521, Training Accuracy= 0.982\n",
            "Step 495, Minibatch Loss= 0.0583, Training Accuracy= 0.979\n",
            "Step 496, Minibatch Loss= 0.0525, Training Accuracy= 0.983\n",
            "Step 497, Minibatch Loss= 0.0483, Training Accuracy= 0.987\n",
            "Step 498, Minibatch Loss= 0.0600, Training Accuracy= 0.975\n",
            "Step 499, Minibatch Loss= 0.0573, Training Accuracy= 0.979\n",
            "Step 500, Minibatch Loss= 0.0573, Training Accuracy= 0.983\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.8313539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "gSOGx2nYRsKq",
        "outputId": "b95332bf-ae67-49ac-84f5-1f1111341e7b"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-0dde94a30ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuKnOSRw70wV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5483efc-ae65-465c-a54a-ec141caf8350"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}