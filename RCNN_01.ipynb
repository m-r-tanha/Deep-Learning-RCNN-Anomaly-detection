{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCNN.01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-r-tanha/RCNN-Anomaly-detection/blob/master/RCNN_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg8fUysVwtBU",
        "colab_type": "code",
        "outputId": "7bc16c9a-b7ca-4fc2-d6d0-828c2bafe1e1",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import scale, normalize, minmax_scale\n",
        "from tensorflow.contrib import rnn\n",
        "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import io\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "tbc=TensorBoardColab()\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://4ecf94bc.ngrok.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-287f42ae-50ff-47fb-a818-ef72ca87e7cf\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-287f42ae-50ff-47fb-a818-ef72ca87e7cf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 1000_Sample2.xlsx to 1000_Sample2.xlsx\n",
            "User uploaded file \"1000_Sample2.xlsx\" with length 235213 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBff4QmxgQdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = pd.ExcelWriter('Yexcel.xlsx')\n",
        "data_path=\"1000_Sample2.xlsx\"\n",
        "data=pd.read_excel(data_path,sheet_name='Sheet3')\n",
        "data=data.fillna(0.0)\n",
        "data.rename(index=data.Cell,inplace=True)\n",
        "data.drop('Cell',axis=1,inplace=True)\n",
        "data.Lable_target=data.Lable_target.replace(to_replace=['N','DS','SI','GD','SD','GI'],value=[1,2,3,4,5,6])\n",
        "X=data.drop('Lable_target',axis=1).values\n",
        "Y=data.Lable_target\n",
        "#print(Y)\n",
        "#Y.describe()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffn-zENWLcpY",
        "colab_type": "code",
        "outputId": "afb3b389-db5b-4dc3-f6f2-0a0e2363dbfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.1600e+03 2.9400e+03 2.5300e+03 ... 4.0000e+03 4.0800e+03 3.1200e+03]\n",
            " [1.0000e+00 1.0000e+00 1.0000e+00 ... 1.2089e+02 1.2081e+02 6.6960e+01]\n",
            " [1.7940e+01 1.8560e+01 1.1290e+01 ... 1.3840e+01 1.3120e+01 8.5600e+00]\n",
            " ...\n",
            " [2.4770e+01 1.7700e+01 1.6510e+01 ... 1.7420e+01 1.6630e+01 1.8010e+01]\n",
            " [1.5300e+00 1.8400e+00 1.4000e+00 ... 2.4000e-01 2.8000e-01 3.1000e-01]\n",
            " [4.7200e+00 2.5100e+00 3.0800e+00 ... 5.0500e+00 6.4800e+00 5.9800e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7XsuA8Py-o6",
        "colab_type": "code",
        "outputId": "2f6912f3-d445-4ab4-fa1f-475db67ceb49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# creating a noise with the same dimension as the dataset \n",
        "mu, sigma = .75, 0.1 \n",
        "noise = np.random.normal(mu, sigma, [1000,31]) \n",
        "noise.shape\n",
        "#-------------\n",
        "\n",
        "wn = np.random.randn(len(X),31)\n",
        "data_wn=X+wn\n",
        "\n",
        "X=np.append (X,data_wn,axis=0)\n",
        "Y=np.append(Y,Y,axis=0)\n",
        "print(X)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3.16000000e+03  2.94000000e+03  2.53000000e+03 ...  4.00000000e+03\n",
            "   4.08000000e+03  3.12000000e+03]\n",
            " [ 1.00000000e+00  1.00000000e+00  1.00000000e+00 ...  1.20890000e+02\n",
            "   1.20810000e+02  6.69600000e+01]\n",
            " [ 1.79400000e+01  1.85600000e+01  1.12900000e+01 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " ...\n",
            " [ 2.64238070e+01  1.75011617e+01  1.55326041e+01 ...  1.87181984e+01\n",
            "   1.71298591e+01  1.89705630e+01]\n",
            " [ 7.47556802e-01  2.71514128e-01  6.48289901e-01 ...  9.07078129e-01\n",
            "   2.56023517e-01 -2.78646657e-01]\n",
            " [ 3.16108431e+00  2.08930694e+00  3.32417547e+00 ...  4.67366973e+00\n",
            "   6.57715614e+00  5.97521840e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Jj38wfw8KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Normalized_X=scale(X, axis=1)\n",
        "Mi=Normalized_X.min()   \n",
        "Normalized_X=(-1*Mi)+Normalized_X\n",
        "\n",
        "\n",
        "X_train=Normalized_X[0:1400]\n",
        "X_test=Normalized_X[1401:1999]\n",
        "\n",
        "Y_train=Y[0:1400]\n",
        "Y_test=Y[1401:1999]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKUzIFjDLK9i",
        "colab_type": "code",
        "outputId": "af654c0a-99f7-4c8c-b5d7-8f04322b177d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.utils import np_utils\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "\n",
        "Y_train = np.delete(Y_train , 0, 1)\n",
        "Y_test = np.delete(Y_test , 0, 1)\n",
        "# Training Parametersznum_steps = 30\n",
        "num_steps=700\n",
        "batch_size = 120\n",
        "display_step = 1\n",
        "strides = 1\n",
        "k = 1\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 31  #  data input (img shape: 28*28)\n",
        "num_hidden = 100\n",
        "num_classes = 6  #  total classes (0-9 digits)\n",
        "dropout = 0.7  # Dropout, probability to keep units\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [None, num_input])\n",
        "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)\n",
        "\n",
        "is_training = tf.placeholder(tf.bool, name='MODE')\n",
        "#is_training=True\n",
        "\n",
        "# Batch Normal\n",
        "def batch_norm_layer(inputT, is_training=True, scope=None):\n",
        "    # Note: is_training is tf.placeholder(tf.bool) type\n",
        "    return tf.cond(is_training,\n",
        "                    lambda: batch_norm(inputT, is_training=True,\n",
        "                    center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope=scope),\n",
        "                    lambda: batch_norm(inputT, is_training=False,\n",
        "                    center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9,\n",
        "                    scope=scope, reuse = True))\n",
        "#\n",
        "# Store layers weight & bias\n",
        "# The first three convolutional layer\n",
        "w_c_1 = tf.Variable(tf.random_normal([1, 3, 1, 28]))\n",
        "w_c_2 = tf.Variable(tf.random_normal([1, 3, 28, 56]))\n",
        "w_c_3 = tf.Variable(tf.random_normal([1, 3, 56, 112]))\n",
        "b_c_1 = tf.Variable(tf.zeros([28]))\n",
        "b_c_2 = tf.Variable(tf.zeros([56]))\n",
        "b_c_3 = tf.Variable(tf.zeros([112]))\n",
        "\n",
        "# The second three convolutional layer weights\n",
        "w_c_4 = tf.Variable(tf.random_normal([1, 3, 112, 224]))\n",
        "w_c_5 = tf.Variable(tf.random_normal([1, 3, 224, 448]))\n",
        "w_c_6 = tf.Variable(tf.random_normal([1, 3, 448, 896]))\n",
        "b_c_4 = tf.Variable(tf.zeros([224]))\n",
        "b_c_5 = tf.Variable(tf.zeros([448]))\n",
        "b_c_6 = tf.Variable(tf.zeros([896]))\n",
        "\n",
        "# Fully connected weight\n",
        "w_f_1 = tf.Variable(tf.random_normal([1 * 31 * 896, 1792])) # fully connected, 1*3*896 inputs, 2048 outputs\n",
        "w_f_2 = tf.Variable(tf.random_normal([1792, 896]))\n",
        "w_f_3 = tf.Variable(tf.random_normal([896, 448]))\n",
        "b_f_1 = tf.Variable(tf.zeros([1792]))\n",
        "b_f_2 = tf.Variable(tf.zeros([896]))\n",
        "b_f_3 = tf.Variable(tf.zeros([448]))\n",
        "\n",
        "# output layer weight\n",
        "w_out = tf.Variable(tf.random_normal([448, num_classes]))\n",
        "b_out = tf.Variable(tf.zeros([num_classes]))\n",
        "\n",
        "#\n",
        "# Define model\n",
        "x = tf.reshape(X, shape=[-1, 1, 31, 1])\n",
        "\n",
        "# first layer convolution\n",
        "conv1 = tf.nn.conv2d(x, w_c_1, strides=[1, 1, 1, 1], padding='SAME') + b_c_1\n",
        "conv1=batch_norm(conv1, is_training=True,center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope='conv1_bn')\n",
        "conv1 = tf.nn.relu(conv1)\n",
        "\n",
        "\n",
        "# second layer convolution\n",
        "conv2 = tf.nn.conv2d(conv1, w_c_2, strides=[1, strides, strides, 1], padding='SAME') + b_c_2\n",
        "conv2=batch_norm(conv2, is_training=True,center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope='conv2_bn')\n",
        "conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "# third layer convolution\n",
        "conv3 = tf.nn.conv2d(conv2, w_c_3, strides=[1, strides, strides, 1], padding='SAME') + b_c_3\n",
        "conv3=batch_norm(conv3, is_training=True,center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope='conv3_bn')\n",
        "conv3 = tf.nn.relu(conv3)\n",
        "\n",
        "# first Max Pooling (down-sampling)\n",
        "pool_1 = tf.nn.max_pool(conv3, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
        "\n",
        "# fourth layer convolution\n",
        "conv4 = tf.nn.conv2d(pool_1, w_c_4, strides=[1, strides, strides, 1], padding='SAME') + b_c_4\n",
        "conv4=batch_norm(conv4, is_training=True,center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope='conv4_bn')\n",
        "conv4 = tf.nn.relu(conv4)\n",
        "conv4 = tf.nn.dropout(conv4, dropout)\n",
        "\n",
        "\n",
        "# fifth layer convolution\n",
        "conv5 = tf.nn.conv2d(conv4, w_c_5, strides=[1, strides, strides, 1], padding='SAME') + b_c_5\n",
        "conv5=batch_norm(conv5, is_training=True,center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope='conv5_bn')\n",
        "conv5 = tf.nn.relu(conv5)\n",
        "\n",
        "# sixth layer convolution\n",
        "conv6 = tf.nn.conv2d(conv5, w_c_6, strides=[1, strides, strides, 1], padding='SAME') + b_c_6\n",
        "conv6=batch_norm(conv6, is_training=True,center=True, scale=True, activation_fn=tf.nn.relu, decay=0.9, scope='conv6_bn')\n",
        "conv6 = tf.nn.relu(conv6)\n",
        "#conv6 = tf.nn.dropout(conv6, dropout)\n",
        "\n",
        "# second Max Pooling (down-sampling)\n",
        "pool_2 = tf.nn.max_pool(conv6, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
        "\n",
        "pool_2 = tf.reshape(pool_2, [-1,31,896])\n",
        "\n",
        "# Define weights\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([num_hidden,num_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}\n",
        "\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    lstm_cell = rnn.BasicLSTMCell(num_hidden,reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, x , dtype=tf.float32)\n",
        "\n",
        "    return tf.matmul(outputs[:,-1], weights['out']) + biases['out']\n",
        "\n",
        "logits = RNN(pool_2, weights, biases)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$4\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#extra\n",
        "\n",
        "#rec, rec_op = tf.metrics.recall(labels=tf.argmax(Y, 1), predictions=tf.argmax(prediction, 1))\n",
        "#pre, pre_op = tf.metrics.precision(labels=tf.argmax(Y, 1), predictions=tf.argmax(prediction, 1))\n",
        "#rec pre\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Please don't change these.\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Tensorboard\n",
        "writer=tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
        "#tbc.save_image(title=\"test_title\")\n",
        "\n",
        "writer.close()\n",
        "A=pd.DataFrame({'loss':[], 'Acc':[]})\n",
        "\n",
        "# Start training\n",
        "with tf.Session(config=config) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, num_steps + 1):\n",
        "\n",
        "        sess.run(train_op, feed_dict={X: X_train, Y: Y_train, keep_prob: 0.7})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc= sess.run([loss_op, accuracy], feed_dict={X: X_train,\n",
        "                                                                 Y: Y_train,\n",
        "                                                                 keep_prob: 1.0})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc) )\n",
        "            df=pd.DataFrame({'loss':[loss], 'Acc':[acc]})\n",
        "            A=A.append(df) \n",
        "    saver=tf.train.Saver()\n",
        "    save_path=saver.save(sess,\"./TF_ModelwBN_for_Paper4.ckpt\")\n",
        "    tbc.save_value(\"graph_name\", \"line_name\", num_steps, loss)\n",
        "    #tbc.flush_line(line_name)\n",
        "    tbc.close()\n",
        "    print(\"Optimization Finished!\")    \n",
        "    print(\"Testing Accuracy:\", \\\n",
        "          sess.run(accuracy, feed_dict={X: X_test , Y: Y_test}))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:650: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-8-a9169dd2db2b>:94: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-8-a9169dd2db2b>:124: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-8-a9169dd2db2b>:126: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-8-a9169dd2db2b>:135: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Step 1, Minibatch Loss= 7.5704, Training Accuracy= 0.249\n",
            "Step 2, Minibatch Loss= 6.4777, Training Accuracy= 0.276\n",
            "Step 3, Minibatch Loss= 4.0126, Training Accuracy= 0.249\n",
            "Step 4, Minibatch Loss= 1.9351, Training Accuracy= 0.309\n",
            "Step 5, Minibatch Loss= 1.4016, Training Accuracy= 0.460\n",
            "Step 6, Minibatch Loss= 2.0854, Training Accuracy= 0.387\n",
            "Step 7, Minibatch Loss= 1.9551, Training Accuracy= 0.396\n",
            "Step 8, Minibatch Loss= 1.5407, Training Accuracy= 0.471\n",
            "Step 9, Minibatch Loss= 1.3333, Training Accuracy= 0.541\n",
            "Step 10, Minibatch Loss= 1.3635, Training Accuracy= 0.503\n",
            "Step 11, Minibatch Loss= 1.3517, Training Accuracy= 0.482\n",
            "Step 12, Minibatch Loss= 1.3556, Training Accuracy= 0.452\n",
            "Step 13, Minibatch Loss= 1.3186, Training Accuracy= 0.445\n",
            "Step 14, Minibatch Loss= 1.2258, Training Accuracy= 0.475\n",
            "Step 15, Minibatch Loss= 1.1292, Training Accuracy= 0.527\n",
            "Step 16, Minibatch Loss= 1.0663, Training Accuracy= 0.558\n",
            "Step 17, Minibatch Loss= 1.0518, Training Accuracy= 0.569\n",
            "Step 18, Minibatch Loss= 1.0798, Training Accuracy= 0.558\n",
            "Step 19, Minibatch Loss= 1.1230, Training Accuracy= 0.543\n",
            "Step 20, Minibatch Loss= 1.1069, Training Accuracy= 0.559\n",
            "Step 21, Minibatch Loss= 1.0842, Training Accuracy= 0.566\n",
            "Step 22, Minibatch Loss= 1.0730, Training Accuracy= 0.566\n",
            "Step 23, Minibatch Loss= 1.0168, Training Accuracy= 0.580\n",
            "Step 24, Minibatch Loss= 0.9802, Training Accuracy= 0.591\n",
            "Step 25, Minibatch Loss= 0.9661, Training Accuracy= 0.582\n",
            "Step 26, Minibatch Loss= 0.9653, Training Accuracy= 0.599\n",
            "Step 27, Minibatch Loss= 0.9485, Training Accuracy= 0.602\n",
            "Step 28, Minibatch Loss= 0.9549, Training Accuracy= 0.600\n",
            "Step 29, Minibatch Loss= 0.9419, Training Accuracy= 0.595\n",
            "Step 30, Minibatch Loss= 0.9328, Training Accuracy= 0.605\n",
            "Step 31, Minibatch Loss= 0.8935, Training Accuracy= 0.625\n",
            "Step 32, Minibatch Loss= 0.8820, Training Accuracy= 0.635\n",
            "Step 33, Minibatch Loss= 0.8882, Training Accuracy= 0.619\n",
            "Step 34, Minibatch Loss= 0.8729, Training Accuracy= 0.628\n",
            "Step 35, Minibatch Loss= 0.8530, Training Accuracy= 0.645\n",
            "Step 36, Minibatch Loss= 0.8522, Training Accuracy= 0.649\n",
            "Step 37, Minibatch Loss= 0.8456, Training Accuracy= 0.644\n",
            "Step 38, Minibatch Loss= 0.8335, Training Accuracy= 0.651\n",
            "Step 39, Minibatch Loss= 0.8320, Training Accuracy= 0.651\n",
            "Step 40, Minibatch Loss= 0.8056, Training Accuracy= 0.671\n",
            "Step 41, Minibatch Loss= 0.7961, Training Accuracy= 0.684\n",
            "Step 42, Minibatch Loss= 0.7868, Training Accuracy= 0.674\n",
            "Step 43, Minibatch Loss= 0.8064, Training Accuracy= 0.671\n",
            "Step 44, Minibatch Loss= 0.7760, Training Accuracy= 0.679\n",
            "Step 45, Minibatch Loss= 0.7822, Training Accuracy= 0.671\n",
            "Step 46, Minibatch Loss= 0.7831, Training Accuracy= 0.676\n",
            "Step 47, Minibatch Loss= 0.7735, Training Accuracy= 0.682\n",
            "Step 48, Minibatch Loss= 0.7597, Training Accuracy= 0.689\n",
            "Step 49, Minibatch Loss= 0.7587, Training Accuracy= 0.685\n",
            "Step 50, Minibatch Loss= 0.7503, Training Accuracy= 0.695\n",
            "Step 51, Minibatch Loss= 0.7656, Training Accuracy= 0.678\n",
            "Step 52, Minibatch Loss= 0.7320, Training Accuracy= 0.707\n",
            "Step 53, Minibatch Loss= 0.7458, Training Accuracy= 0.700\n",
            "Step 54, Minibatch Loss= 0.7226, Training Accuracy= 0.721\n",
            "Step 55, Minibatch Loss= 0.7160, Training Accuracy= 0.724\n",
            "Step 56, Minibatch Loss= 0.7131, Training Accuracy= 0.718\n",
            "Step 57, Minibatch Loss= 0.7148, Training Accuracy= 0.722\n",
            "Step 58, Minibatch Loss= 0.7115, Training Accuracy= 0.709\n",
            "Step 59, Minibatch Loss= 0.6926, Training Accuracy= 0.729\n",
            "Step 60, Minibatch Loss= 0.6932, Training Accuracy= 0.728\n",
            "Step 61, Minibatch Loss= 0.6941, Training Accuracy= 0.730\n",
            "Step 62, Minibatch Loss= 0.6967, Training Accuracy= 0.723\n",
            "Step 63, Minibatch Loss= 0.6804, Training Accuracy= 0.730\n",
            "Step 64, Minibatch Loss= 0.6694, Training Accuracy= 0.729\n",
            "Step 65, Minibatch Loss= 0.6625, Training Accuracy= 0.738\n",
            "Step 66, Minibatch Loss= 0.6677, Training Accuracy= 0.735\n",
            "Step 67, Minibatch Loss= 0.6572, Training Accuracy= 0.743\n",
            "Step 68, Minibatch Loss= 0.6567, Training Accuracy= 0.735\n",
            "Step 69, Minibatch Loss= 0.6520, Training Accuracy= 0.749\n",
            "Step 70, Minibatch Loss= 0.6463, Training Accuracy= 0.747\n",
            "Step 71, Minibatch Loss= 0.6288, Training Accuracy= 0.762\n",
            "Step 72, Minibatch Loss= 0.6442, Training Accuracy= 0.753\n",
            "Step 73, Minibatch Loss= 0.6307, Training Accuracy= 0.756\n",
            "Step 74, Minibatch Loss= 0.6150, Training Accuracy= 0.767\n",
            "Step 75, Minibatch Loss= 0.6264, Training Accuracy= 0.756\n",
            "Step 76, Minibatch Loss= 0.6031, Training Accuracy= 0.785\n",
            "Step 77, Minibatch Loss= 0.6277, Training Accuracy= 0.754\n",
            "Step 78, Minibatch Loss= 0.6150, Training Accuracy= 0.768\n",
            "Step 79, Minibatch Loss= 0.6067, Training Accuracy= 0.763\n",
            "Step 80, Minibatch Loss= 0.6015, Training Accuracy= 0.763\n",
            "Step 81, Minibatch Loss= 0.5932, Training Accuracy= 0.770\n",
            "Step 82, Minibatch Loss= 0.5894, Training Accuracy= 0.765\n",
            "Step 83, Minibatch Loss= 0.5729, Training Accuracy= 0.782\n",
            "Step 84, Minibatch Loss= 0.5831, Training Accuracy= 0.775\n",
            "Step 85, Minibatch Loss= 0.5877, Training Accuracy= 0.772\n",
            "Step 86, Minibatch Loss= 0.5960, Training Accuracy= 0.770\n",
            "Step 87, Minibatch Loss= 0.5742, Training Accuracy= 0.767\n",
            "Step 88, Minibatch Loss= 0.5436, Training Accuracy= 0.803\n",
            "Step 89, Minibatch Loss= 0.5821, Training Accuracy= 0.779\n",
            "Step 90, Minibatch Loss= 0.5591, Training Accuracy= 0.791\n",
            "Step 91, Minibatch Loss= 0.5653, Training Accuracy= 0.783\n",
            "Step 92, Minibatch Loss= 0.5598, Training Accuracy= 0.786\n",
            "Step 93, Minibatch Loss= 0.5532, Training Accuracy= 0.785\n",
            "Step 94, Minibatch Loss= 0.5318, Training Accuracy= 0.794\n",
            "Step 95, Minibatch Loss= 0.5331, Training Accuracy= 0.796\n",
            "Step 96, Minibatch Loss= 0.5496, Training Accuracy= 0.793\n",
            "Step 97, Minibatch Loss= 0.5382, Training Accuracy= 0.797\n",
            "Step 98, Minibatch Loss= 0.5340, Training Accuracy= 0.793\n",
            "Step 99, Minibatch Loss= 0.5265, Training Accuracy= 0.808\n",
            "Step 100, Minibatch Loss= 0.5117, Training Accuracy= 0.808\n",
            "Step 101, Minibatch Loss= 0.5201, Training Accuracy= 0.804\n",
            "Step 102, Minibatch Loss= 0.5204, Training Accuracy= 0.806\n",
            "Step 103, Minibatch Loss= 0.5222, Training Accuracy= 0.802\n",
            "Step 104, Minibatch Loss= 0.5122, Training Accuracy= 0.806\n",
            "Step 105, Minibatch Loss= 0.4942, Training Accuracy= 0.804\n",
            "Step 106, Minibatch Loss= 0.5082, Training Accuracy= 0.811\n",
            "Step 107, Minibatch Loss= 0.5027, Training Accuracy= 0.812\n",
            "Step 108, Minibatch Loss= 0.4915, Training Accuracy= 0.804\n",
            "Step 109, Minibatch Loss= 0.4882, Training Accuracy= 0.816\n",
            "Step 110, Minibatch Loss= 0.4726, Training Accuracy= 0.829\n",
            "Step 111, Minibatch Loss= 0.4857, Training Accuracy= 0.815\n",
            "Step 112, Minibatch Loss= 0.4842, Training Accuracy= 0.814\n",
            "Step 113, Minibatch Loss= 0.4725, Training Accuracy= 0.819\n",
            "Step 114, Minibatch Loss= 0.4681, Training Accuracy= 0.819\n",
            "Step 115, Minibatch Loss= 0.4749, Training Accuracy= 0.819\n",
            "Step 116, Minibatch Loss= 0.4526, Training Accuracy= 0.828\n",
            "Step 117, Minibatch Loss= 0.4425, Training Accuracy= 0.855\n",
            "Step 118, Minibatch Loss= 0.4396, Training Accuracy= 0.839\n",
            "Step 119, Minibatch Loss= 0.4549, Training Accuracy= 0.819\n",
            "Step 120, Minibatch Loss= 0.4525, Training Accuracy= 0.826\n",
            "Step 121, Minibatch Loss= 0.4421, Training Accuracy= 0.824\n",
            "Step 122, Minibatch Loss= 0.4342, Training Accuracy= 0.846\n",
            "Step 123, Minibatch Loss= 0.4368, Training Accuracy= 0.829\n",
            "Step 124, Minibatch Loss= 0.4416, Training Accuracy= 0.839\n",
            "Step 125, Minibatch Loss= 0.4226, Training Accuracy= 0.846\n",
            "Step 126, Minibatch Loss= 0.4166, Training Accuracy= 0.834\n",
            "Step 127, Minibatch Loss= 0.4125, Training Accuracy= 0.842\n",
            "Step 128, Minibatch Loss= 0.4186, Training Accuracy= 0.836\n",
            "Step 129, Minibatch Loss= 0.4185, Training Accuracy= 0.842\n",
            "Step 130, Minibatch Loss= 0.4007, Training Accuracy= 0.851\n",
            "Step 131, Minibatch Loss= 0.3988, Training Accuracy= 0.844\n",
            "Step 132, Minibatch Loss= 0.3883, Training Accuracy= 0.855\n",
            "Step 133, Minibatch Loss= 0.4079, Training Accuracy= 0.849\n",
            "Step 134, Minibatch Loss= 0.4024, Training Accuracy= 0.851\n",
            "Step 135, Minibatch Loss= 0.3965, Training Accuracy= 0.855\n",
            "Step 136, Minibatch Loss= 0.3905, Training Accuracy= 0.856\n",
            "Step 137, Minibatch Loss= 0.4026, Training Accuracy= 0.847\n",
            "Step 138, Minibatch Loss= 0.3812, Training Accuracy= 0.860\n",
            "Step 139, Minibatch Loss= 0.3863, Training Accuracy= 0.856\n",
            "Step 140, Minibatch Loss= 0.3798, Training Accuracy= 0.861\n",
            "Step 141, Minibatch Loss= 0.3726, Training Accuracy= 0.859\n",
            "Step 142, Minibatch Loss= 0.3589, Training Accuracy= 0.872\n",
            "Step 143, Minibatch Loss= 0.3570, Training Accuracy= 0.858\n",
            "Step 144, Minibatch Loss= 0.3623, Training Accuracy= 0.861\n",
            "Step 145, Minibatch Loss= 0.3509, Training Accuracy= 0.866\n",
            "Step 146, Minibatch Loss= 0.3604, Training Accuracy= 0.868\n",
            "Step 147, Minibatch Loss= 0.3453, Training Accuracy= 0.876\n",
            "Step 148, Minibatch Loss= 0.3512, Training Accuracy= 0.864\n",
            "Step 149, Minibatch Loss= 0.3464, Training Accuracy= 0.872\n",
            "Step 150, Minibatch Loss= 0.3401, Training Accuracy= 0.878\n",
            "Step 151, Minibatch Loss= 0.3504, Training Accuracy= 0.869\n",
            "Step 152, Minibatch Loss= 0.3290, Training Accuracy= 0.884\n",
            "Step 153, Minibatch Loss= 0.3328, Training Accuracy= 0.871\n",
            "Step 154, Minibatch Loss= 0.3243, Training Accuracy= 0.874\n",
            "Step 155, Minibatch Loss= 0.3280, Training Accuracy= 0.869\n",
            "Step 156, Minibatch Loss= 0.3185, Training Accuracy= 0.877\n",
            "Step 157, Minibatch Loss= 0.3190, Training Accuracy= 0.879\n",
            "Step 158, Minibatch Loss= 0.3163, Training Accuracy= 0.886\n",
            "Step 159, Minibatch Loss= 0.3046, Training Accuracy= 0.889\n",
            "Step 160, Minibatch Loss= 0.3143, Training Accuracy= 0.876\n",
            "Step 161, Minibatch Loss= 0.3145, Training Accuracy= 0.885\n",
            "Step 162, Minibatch Loss= 0.2905, Training Accuracy= 0.885\n",
            "Step 163, Minibatch Loss= 0.3228, Training Accuracy= 0.879\n",
            "Step 164, Minibatch Loss= 0.2955, Training Accuracy= 0.884\n",
            "Step 165, Minibatch Loss= 0.3149, Training Accuracy= 0.879\n",
            "Step 166, Minibatch Loss= 0.3196, Training Accuracy= 0.881\n",
            "Step 167, Minibatch Loss= 0.3189, Training Accuracy= 0.876\n",
            "Step 168, Minibatch Loss= 0.2931, Training Accuracy= 0.892\n",
            "Step 169, Minibatch Loss= 0.2975, Training Accuracy= 0.879\n",
            "Step 170, Minibatch Loss= 0.2828, Training Accuracy= 0.908\n",
            "Step 171, Minibatch Loss= 0.2936, Training Accuracy= 0.888\n",
            "Step 172, Minibatch Loss= 0.2836, Training Accuracy= 0.895\n",
            "Step 173, Minibatch Loss= 0.2698, Training Accuracy= 0.911\n",
            "Step 174, Minibatch Loss= 0.2745, Training Accuracy= 0.899\n",
            "Step 175, Minibatch Loss= 0.2779, Training Accuracy= 0.890\n",
            "Step 176, Minibatch Loss= 0.2771, Training Accuracy= 0.902\n",
            "Step 177, Minibatch Loss= 0.2571, Training Accuracy= 0.900\n",
            "Step 178, Minibatch Loss= 0.2639, Training Accuracy= 0.906\n",
            "Step 179, Minibatch Loss= 0.2541, Training Accuracy= 0.916\n",
            "Step 180, Minibatch Loss= 0.2550, Training Accuracy= 0.911\n",
            "Step 181, Minibatch Loss= 0.2449, Training Accuracy= 0.916\n",
            "Step 182, Minibatch Loss= 0.2451, Training Accuracy= 0.899\n",
            "Step 183, Minibatch Loss= 0.2429, Training Accuracy= 0.904\n",
            "Step 184, Minibatch Loss= 0.2418, Training Accuracy= 0.907\n",
            "Step 185, Minibatch Loss= 0.2538, Training Accuracy= 0.905\n",
            "Step 186, Minibatch Loss= 0.2414, Training Accuracy= 0.912\n",
            "Step 187, Minibatch Loss= 0.2363, Training Accuracy= 0.913\n",
            "Step 188, Minibatch Loss= 0.2430, Training Accuracy= 0.915\n",
            "Step 189, Minibatch Loss= 0.2446, Training Accuracy= 0.919\n",
            "Step 190, Minibatch Loss= 0.2270, Training Accuracy= 0.918\n",
            "Step 191, Minibatch Loss= 0.2331, Training Accuracy= 0.921\n",
            "Step 192, Minibatch Loss= 0.2258, Training Accuracy= 0.922\n",
            "Step 193, Minibatch Loss= 0.2087, Training Accuracy= 0.930\n",
            "Step 194, Minibatch Loss= 0.2237, Training Accuracy= 0.917\n",
            "Step 195, Minibatch Loss= 0.2247, Training Accuracy= 0.924\n",
            "Step 196, Minibatch Loss= 0.2226, Training Accuracy= 0.914\n",
            "Step 197, Minibatch Loss= 0.2184, Training Accuracy= 0.922\n",
            "Step 198, Minibatch Loss= 0.2029, Training Accuracy= 0.929\n",
            "Step 199, Minibatch Loss= 0.2181, Training Accuracy= 0.925\n",
            "Step 200, Minibatch Loss= 0.2137, Training Accuracy= 0.925\n",
            "Step 201, Minibatch Loss= 0.2014, Training Accuracy= 0.920\n",
            "Step 202, Minibatch Loss= 0.1984, Training Accuracy= 0.928\n",
            "Step 203, Minibatch Loss= 0.1997, Training Accuracy= 0.929\n",
            "Step 204, Minibatch Loss= 0.1943, Training Accuracy= 0.931\n",
            "Step 205, Minibatch Loss= 0.2035, Training Accuracy= 0.931\n",
            "Step 206, Minibatch Loss= 0.1871, Training Accuracy= 0.939\n",
            "Step 207, Minibatch Loss= 0.1813, Training Accuracy= 0.939\n",
            "Step 208, Minibatch Loss= 0.1935, Training Accuracy= 0.932\n",
            "Step 209, Minibatch Loss= 0.1683, Training Accuracy= 0.947\n",
            "Step 210, Minibatch Loss= 0.1941, Training Accuracy= 0.932\n",
            "Step 211, Minibatch Loss= 0.1795, Training Accuracy= 0.936\n",
            "Step 212, Minibatch Loss= 0.1845, Training Accuracy= 0.931\n",
            "Step 213, Minibatch Loss= 0.1965, Training Accuracy= 0.927\n",
            "Step 214, Minibatch Loss= 0.1868, Training Accuracy= 0.937\n",
            "Step 215, Minibatch Loss= 0.1962, Training Accuracy= 0.928\n",
            "Step 216, Minibatch Loss= 0.1761, Training Accuracy= 0.944\n",
            "Step 217, Minibatch Loss= 0.1765, Training Accuracy= 0.939\n",
            "Step 218, Minibatch Loss= 0.1898, Training Accuracy= 0.929\n",
            "Step 219, Minibatch Loss= 0.1861, Training Accuracy= 0.939\n",
            "Step 220, Minibatch Loss= 0.1860, Training Accuracy= 0.930\n",
            "Step 221, Minibatch Loss= 0.1762, Training Accuracy= 0.936\n",
            "Step 222, Minibatch Loss= 0.1943, Training Accuracy= 0.934\n",
            "Step 223, Minibatch Loss= 0.1792, Training Accuracy= 0.936\n",
            "Step 224, Minibatch Loss= 0.1677, Training Accuracy= 0.945\n",
            "Step 225, Minibatch Loss= 0.1666, Training Accuracy= 0.939\n",
            "Step 226, Minibatch Loss= 0.1532, Training Accuracy= 0.954\n",
            "Step 227, Minibatch Loss= 0.1767, Training Accuracy= 0.936\n",
            "Step 228, Minibatch Loss= 0.1620, Training Accuracy= 0.941\n",
            "Step 229, Minibatch Loss= 0.1764, Training Accuracy= 0.939\n",
            "Step 230, Minibatch Loss= 0.1604, Training Accuracy= 0.951\n",
            "Step 231, Minibatch Loss= 0.1423, Training Accuracy= 0.955\n",
            "Step 232, Minibatch Loss= 0.1556, Training Accuracy= 0.954\n",
            "Step 233, Minibatch Loss= 0.1724, Training Accuracy= 0.937\n",
            "Step 234, Minibatch Loss= 0.1505, Training Accuracy= 0.950\n",
            "Step 235, Minibatch Loss= 0.1645, Training Accuracy= 0.945\n",
            "Step 236, Minibatch Loss= 0.1469, Training Accuracy= 0.949\n",
            "Step 237, Minibatch Loss= 0.1466, Training Accuracy= 0.954\n",
            "Step 238, Minibatch Loss= 0.1416, Training Accuracy= 0.953\n",
            "Step 239, Minibatch Loss= 0.1735, Training Accuracy= 0.939\n",
            "Step 240, Minibatch Loss= 0.1334, Training Accuracy= 0.959\n",
            "Step 241, Minibatch Loss= 0.1474, Training Accuracy= 0.952\n",
            "Step 242, Minibatch Loss= 0.1433, Training Accuracy= 0.956\n",
            "Step 243, Minibatch Loss= 0.1507, Training Accuracy= 0.949\n",
            "Step 244, Minibatch Loss= 0.1251, Training Accuracy= 0.958\n",
            "Step 245, Minibatch Loss= 0.1340, Training Accuracy= 0.954\n",
            "Step 246, Minibatch Loss= 0.1268, Training Accuracy= 0.961\n",
            "Step 247, Minibatch Loss= 0.1311, Training Accuracy= 0.959\n",
            "Step 248, Minibatch Loss= 0.1329, Training Accuracy= 0.957\n",
            "Step 249, Minibatch Loss= 0.1285, Training Accuracy= 0.961\n",
            "Step 250, Minibatch Loss= 0.1357, Training Accuracy= 0.948\n",
            "Step 251, Minibatch Loss= 0.1458, Training Accuracy= 0.953\n",
            "Step 252, Minibatch Loss= 0.1198, Training Accuracy= 0.956\n",
            "Step 253, Minibatch Loss= 0.1178, Training Accuracy= 0.966\n",
            "Step 254, Minibatch Loss= 0.1281, Training Accuracy= 0.957\n",
            "Step 255, Minibatch Loss= 0.1133, Training Accuracy= 0.959\n",
            "Step 256, Minibatch Loss= 0.1236, Training Accuracy= 0.957\n",
            "Step 257, Minibatch Loss= 0.1257, Training Accuracy= 0.959\n",
            "Step 258, Minibatch Loss= 0.1213, Training Accuracy= 0.960\n",
            "Step 259, Minibatch Loss= 0.1139, Training Accuracy= 0.961\n",
            "Step 260, Minibatch Loss= 0.1068, Training Accuracy= 0.963\n",
            "Step 261, Minibatch Loss= 0.1144, Training Accuracy= 0.963\n",
            "Step 262, Minibatch Loss= 0.1283, Training Accuracy= 0.951\n",
            "Step 263, Minibatch Loss= 0.1154, Training Accuracy= 0.961\n",
            "Step 264, Minibatch Loss= 0.1222, Training Accuracy= 0.960\n",
            "Step 265, Minibatch Loss= 0.1280, Training Accuracy= 0.960\n",
            "Step 266, Minibatch Loss= 0.1129, Training Accuracy= 0.958\n",
            "Step 267, Minibatch Loss= 0.1096, Training Accuracy= 0.961\n",
            "Step 268, Minibatch Loss= 0.1154, Training Accuracy= 0.959\n",
            "Step 269, Minibatch Loss= 0.1057, Training Accuracy= 0.967\n",
            "Step 270, Minibatch Loss= 0.1099, Training Accuracy= 0.962\n",
            "Step 271, Minibatch Loss= 0.1013, Training Accuracy= 0.971\n",
            "Step 272, Minibatch Loss= 0.1024, Training Accuracy= 0.969\n",
            "Step 273, Minibatch Loss= 0.1074, Training Accuracy= 0.966\n",
            "Step 274, Minibatch Loss= 0.1017, Training Accuracy= 0.969\n",
            "Step 275, Minibatch Loss= 0.1033, Training Accuracy= 0.965\n",
            "Step 276, Minibatch Loss= 0.1049, Training Accuracy= 0.966\n",
            "Step 277, Minibatch Loss= 0.1086, Training Accuracy= 0.966\n",
            "Step 278, Minibatch Loss= 0.0953, Training Accuracy= 0.969\n",
            "Step 279, Minibatch Loss= 0.0843, Training Accuracy= 0.975\n",
            "Step 280, Minibatch Loss= 0.0999, Training Accuracy= 0.965\n",
            "Step 281, Minibatch Loss= 0.1058, Training Accuracy= 0.966\n",
            "Step 282, Minibatch Loss= 0.0918, Training Accuracy= 0.971\n",
            "Step 283, Minibatch Loss= 0.1004, Training Accuracy= 0.969\n",
            "Step 284, Minibatch Loss= 0.0943, Training Accuracy= 0.973\n",
            "Step 285, Minibatch Loss= 0.0831, Training Accuracy= 0.979\n",
            "Step 286, Minibatch Loss= 0.0913, Training Accuracy= 0.973\n",
            "Step 287, Minibatch Loss= 0.0860, Training Accuracy= 0.971\n",
            "Step 288, Minibatch Loss= 0.0834, Training Accuracy= 0.974\n",
            "Step 289, Minibatch Loss= 0.0833, Training Accuracy= 0.975\n",
            "Step 290, Minibatch Loss= 0.0865, Training Accuracy= 0.972\n",
            "Step 291, Minibatch Loss= 0.0769, Training Accuracy= 0.978\n",
            "Step 292, Minibatch Loss= 0.0779, Training Accuracy= 0.976\n",
            "Step 293, Minibatch Loss= 0.0748, Training Accuracy= 0.979\n",
            "Step 294, Minibatch Loss= 0.0952, Training Accuracy= 0.969\n",
            "Step 295, Minibatch Loss= 0.0801, Training Accuracy= 0.976\n",
            "Step 296, Minibatch Loss= 0.0876, Training Accuracy= 0.970\n",
            "Step 297, Minibatch Loss= 0.0795, Training Accuracy= 0.976\n",
            "Step 298, Minibatch Loss= 0.0866, Training Accuracy= 0.970\n",
            "Step 299, Minibatch Loss= 0.0791, Training Accuracy= 0.976\n",
            "Step 300, Minibatch Loss= 0.1001, Training Accuracy= 0.966\n",
            "Step 301, Minibatch Loss= 0.0864, Training Accuracy= 0.978\n",
            "Step 302, Minibatch Loss= 0.0707, Training Accuracy= 0.981\n",
            "Step 303, Minibatch Loss= 0.0912, Training Accuracy= 0.971\n",
            "Step 304, Minibatch Loss= 0.0882, Training Accuracy= 0.974\n",
            "Step 305, Minibatch Loss= 0.0840, Training Accuracy= 0.971\n",
            "Step 306, Minibatch Loss= 0.0742, Training Accuracy= 0.978\n",
            "Step 307, Minibatch Loss= 0.0881, Training Accuracy= 0.974\n",
            "Step 308, Minibatch Loss= 0.0839, Training Accuracy= 0.973\n",
            "Step 309, Minibatch Loss= 0.0925, Training Accuracy= 0.967\n",
            "Step 310, Minibatch Loss= 0.0824, Training Accuracy= 0.974\n",
            "Step 311, Minibatch Loss= 0.0859, Training Accuracy= 0.974\n",
            "Step 312, Minibatch Loss= 0.0779, Training Accuracy= 0.974\n",
            "Step 313, Minibatch Loss= 0.0741, Training Accuracy= 0.976\n",
            "Step 314, Minibatch Loss= 0.0911, Training Accuracy= 0.968\n",
            "Step 315, Minibatch Loss= 0.0941, Training Accuracy= 0.968\n",
            "Step 316, Minibatch Loss= 0.0853, Training Accuracy= 0.970\n",
            "Step 317, Minibatch Loss= 0.0919, Training Accuracy= 0.964\n",
            "Step 318, Minibatch Loss= 0.0657, Training Accuracy= 0.984\n",
            "Step 319, Minibatch Loss= 0.0828, Training Accuracy= 0.971\n",
            "Step 320, Minibatch Loss= 0.0797, Training Accuracy= 0.970\n",
            "Step 321, Minibatch Loss= 0.0766, Training Accuracy= 0.977\n",
            "Step 322, Minibatch Loss= 0.0656, Training Accuracy= 0.979\n",
            "Step 323, Minibatch Loss= 0.0694, Training Accuracy= 0.979\n",
            "Step 324, Minibatch Loss= 0.0619, Training Accuracy= 0.984\n",
            "Step 325, Minibatch Loss= 0.0826, Training Accuracy= 0.972\n",
            "Step 326, Minibatch Loss= 0.0734, Training Accuracy= 0.976\n",
            "Step 327, Minibatch Loss= 0.0627, Training Accuracy= 0.983\n",
            "Step 328, Minibatch Loss= 0.0721, Training Accuracy= 0.981\n",
            "Step 329, Minibatch Loss= 0.0608, Training Accuracy= 0.981\n",
            "Step 330, Minibatch Loss= 0.0578, Training Accuracy= 0.986\n",
            "Step 331, Minibatch Loss= 0.0525, Training Accuracy= 0.986\n",
            "Step 332, Minibatch Loss= 0.0517, Training Accuracy= 0.990\n",
            "Step 333, Minibatch Loss= 0.0620, Training Accuracy= 0.981\n",
            "Step 334, Minibatch Loss= 0.0521, Training Accuracy= 0.984\n",
            "Step 335, Minibatch Loss= 0.0541, Training Accuracy= 0.987\n",
            "Step 336, Minibatch Loss= 0.0562, Training Accuracy= 0.983\n",
            "Step 337, Minibatch Loss= 0.0545, Training Accuracy= 0.985\n",
            "Step 338, Minibatch Loss= 0.0603, Training Accuracy= 0.981\n",
            "Step 339, Minibatch Loss= 0.0518, Training Accuracy= 0.988\n",
            "Step 340, Minibatch Loss= 0.0612, Training Accuracy= 0.985\n",
            "Step 341, Minibatch Loss= 0.0438, Training Accuracy= 0.991\n",
            "Step 342, Minibatch Loss= 0.0609, Training Accuracy= 0.980\n",
            "Step 343, Minibatch Loss= 0.0650, Training Accuracy= 0.980\n",
            "Step 344, Minibatch Loss= 0.0540, Training Accuracy= 0.986\n",
            "Step 345, Minibatch Loss= 0.0578, Training Accuracy= 0.986\n",
            "Step 346, Minibatch Loss= 0.0578, Training Accuracy= 0.983\n",
            "Step 347, Minibatch Loss= 0.0480, Training Accuracy= 0.986\n",
            "Step 348, Minibatch Loss= 0.0461, Training Accuracy= 0.987\n",
            "Step 349, Minibatch Loss= 0.0564, Training Accuracy= 0.984\n",
            "Step 350, Minibatch Loss= 0.0590, Training Accuracy= 0.981\n",
            "Step 351, Minibatch Loss= 0.0459, Training Accuracy= 0.988\n",
            "Step 352, Minibatch Loss= 0.0517, Training Accuracy= 0.988\n",
            "Step 353, Minibatch Loss= 0.0460, Training Accuracy= 0.988\n",
            "Step 354, Minibatch Loss= 0.0496, Training Accuracy= 0.987\n",
            "Step 355, Minibatch Loss= 0.0548, Training Accuracy= 0.984\n",
            "Step 356, Minibatch Loss= 0.0614, Training Accuracy= 0.977\n",
            "Step 357, Minibatch Loss= 0.0397, Training Accuracy= 0.990\n",
            "Step 358, Minibatch Loss= 0.0547, Training Accuracy= 0.982\n",
            "Step 359, Minibatch Loss= 0.0460, Training Accuracy= 0.990\n",
            "Step 360, Minibatch Loss= 0.0550, Training Accuracy= 0.985\n",
            "Step 361, Minibatch Loss= 0.0527, Training Accuracy= 0.985\n",
            "Step 362, Minibatch Loss= 0.0456, Training Accuracy= 0.989\n",
            "Step 363, Minibatch Loss= 0.0498, Training Accuracy= 0.986\n",
            "Step 364, Minibatch Loss= 0.0499, Training Accuracy= 0.986\n",
            "Step 365, Minibatch Loss= 0.0426, Training Accuracy= 0.993\n",
            "Step 366, Minibatch Loss= 0.0530, Training Accuracy= 0.983\n",
            "Step 367, Minibatch Loss= 0.0483, Training Accuracy= 0.988\n",
            "Step 368, Minibatch Loss= 0.0548, Training Accuracy= 0.984\n",
            "Step 369, Minibatch Loss= 0.0482, Training Accuracy= 0.989\n",
            "Step 370, Minibatch Loss= 0.0627, Training Accuracy= 0.981\n",
            "Step 371, Minibatch Loss= 0.0496, Training Accuracy= 0.982\n",
            "Step 372, Minibatch Loss= 0.0450, Training Accuracy= 0.985\n",
            "Step 373, Minibatch Loss= 0.0567, Training Accuracy= 0.980\n",
            "Step 374, Minibatch Loss= 0.0486, Training Accuracy= 0.989\n",
            "Step 375, Minibatch Loss= 0.0607, Training Accuracy= 0.985\n",
            "Step 376, Minibatch Loss= 0.0515, Training Accuracy= 0.986\n",
            "Step 377, Minibatch Loss= 0.0602, Training Accuracy= 0.981\n",
            "Step 378, Minibatch Loss= 0.0482, Training Accuracy= 0.984\n",
            "Step 379, Minibatch Loss= 0.0642, Training Accuracy= 0.981\n",
            "Step 380, Minibatch Loss= 0.0357, Training Accuracy= 0.992\n",
            "Step 381, Minibatch Loss= 0.0484, Training Accuracy= 0.985\n",
            "Step 382, Minibatch Loss= 0.0457, Training Accuracy= 0.988\n",
            "Step 383, Minibatch Loss= 0.0412, Training Accuracy= 0.989\n",
            "Step 384, Minibatch Loss= 0.0452, Training Accuracy= 0.991\n",
            "Step 385, Minibatch Loss= 0.0395, Training Accuracy= 0.993\n",
            "Step 386, Minibatch Loss= 0.0352, Training Accuracy= 0.991\n",
            "Step 387, Minibatch Loss= 0.0485, Training Accuracy= 0.985\n",
            "Step 388, Minibatch Loss= 0.0524, Training Accuracy= 0.986\n",
            "Step 389, Minibatch Loss= 0.0362, Training Accuracy= 0.992\n",
            "Step 390, Minibatch Loss= 0.0461, Training Accuracy= 0.983\n",
            "Step 391, Minibatch Loss= 0.0407, Training Accuracy= 0.991\n",
            "Step 392, Minibatch Loss= 0.0296, Training Accuracy= 0.996\n",
            "Step 393, Minibatch Loss= 0.0336, Training Accuracy= 0.992\n",
            "Step 394, Minibatch Loss= 0.0374, Training Accuracy= 0.986\n",
            "Step 395, Minibatch Loss= 0.0434, Training Accuracy= 0.987\n",
            "Step 396, Minibatch Loss= 0.0371, Training Accuracy= 0.992\n",
            "Step 397, Minibatch Loss= 0.0475, Training Accuracy= 0.984\n",
            "Step 398, Minibatch Loss= 0.0385, Training Accuracy= 0.991\n",
            "Step 399, Minibatch Loss= 0.0306, Training Accuracy= 0.996\n",
            "Step 400, Minibatch Loss= 0.0373, Training Accuracy= 0.992\n",
            "Step 401, Minibatch Loss= 0.0456, Training Accuracy= 0.987\n",
            "Step 402, Minibatch Loss= 0.0418, Training Accuracy= 0.988\n",
            "Step 403, Minibatch Loss= 0.0347, Training Accuracy= 0.991\n",
            "Step 404, Minibatch Loss= 0.0351, Training Accuracy= 0.993\n",
            "Step 405, Minibatch Loss= 0.0394, Training Accuracy= 0.989\n",
            "Step 406, Minibatch Loss= 0.0303, Training Accuracy= 0.993\n",
            "Step 407, Minibatch Loss= 0.0458, Training Accuracy= 0.986\n",
            "Step 408, Minibatch Loss= 0.0331, Training Accuracy= 0.991\n",
            "Step 409, Minibatch Loss= 0.0369, Training Accuracy= 0.989\n",
            "Step 410, Minibatch Loss= 0.0333, Training Accuracy= 0.987\n",
            "Step 411, Minibatch Loss= 0.0433, Training Accuracy= 0.986\n",
            "Step 412, Minibatch Loss= 0.0287, Training Accuracy= 0.994\n",
            "Step 413, Minibatch Loss= 0.0391, Training Accuracy= 0.986\n",
            "Step 414, Minibatch Loss= 0.0345, Training Accuracy= 0.989\n",
            "Step 415, Minibatch Loss= 0.0336, Training Accuracy= 0.990\n",
            "Step 416, Minibatch Loss= 0.0345, Training Accuracy= 0.990\n",
            "Step 417, Minibatch Loss= 0.0379, Training Accuracy= 0.986\n",
            "Step 418, Minibatch Loss= 0.0290, Training Accuracy= 0.993\n",
            "Step 419, Minibatch Loss= 0.0314, Training Accuracy= 0.991\n",
            "Step 420, Minibatch Loss= 0.0371, Training Accuracy= 0.988\n",
            "Step 421, Minibatch Loss= 0.0323, Training Accuracy= 0.992\n",
            "Step 422, Minibatch Loss= 0.0313, Training Accuracy= 0.994\n",
            "Step 423, Minibatch Loss= 0.0324, Training Accuracy= 0.991\n",
            "Step 424, Minibatch Loss= 0.0258, Training Accuracy= 0.994\n",
            "Step 425, Minibatch Loss= 0.0350, Training Accuracy= 0.989\n",
            "Step 426, Minibatch Loss= 0.0379, Training Accuracy= 0.990\n",
            "Step 427, Minibatch Loss= 0.0292, Training Accuracy= 0.992\n",
            "Step 428, Minibatch Loss= 0.0358, Training Accuracy= 0.988\n",
            "Step 429, Minibatch Loss= 0.0333, Training Accuracy= 0.994\n",
            "Step 430, Minibatch Loss= 0.0302, Training Accuracy= 0.992\n",
            "Step 431, Minibatch Loss= 0.0344, Training Accuracy= 0.991\n",
            "Step 432, Minibatch Loss= 0.0285, Training Accuracy= 0.994\n",
            "Step 433, Minibatch Loss= 0.0302, Training Accuracy= 0.989\n",
            "Step 434, Minibatch Loss= 0.0235, Training Accuracy= 0.996\n",
            "Step 435, Minibatch Loss= 0.0315, Training Accuracy= 0.991\n",
            "Step 436, Minibatch Loss= 0.0311, Training Accuracy= 0.991\n",
            "Step 437, Minibatch Loss= 0.0348, Training Accuracy= 0.989\n",
            "Step 438, Minibatch Loss= 0.0311, Training Accuracy= 0.992\n",
            "Step 439, Minibatch Loss= 0.0341, Training Accuracy= 0.993\n",
            "Step 440, Minibatch Loss= 0.0254, Training Accuracy= 0.993\n",
            "Step 441, Minibatch Loss= 0.0265, Training Accuracy= 0.993\n",
            "Step 442, Minibatch Loss= 0.0264, Training Accuracy= 0.994\n",
            "Step 443, Minibatch Loss= 0.0238, Training Accuracy= 0.996\n",
            "Step 444, Minibatch Loss= 0.0339, Training Accuracy= 0.989\n",
            "Step 445, Minibatch Loss= 0.0204, Training Accuracy= 0.996\n",
            "Step 446, Minibatch Loss= 0.0238, Training Accuracy= 0.996\n",
            "Step 447, Minibatch Loss= 0.0319, Training Accuracy= 0.991\n",
            "Step 448, Minibatch Loss= 0.0361, Training Accuracy= 0.989\n",
            "Step 449, Minibatch Loss= 0.0216, Training Accuracy= 0.996\n",
            "Step 450, Minibatch Loss= 0.0277, Training Accuracy= 0.995\n",
            "Step 451, Minibatch Loss= 0.0264, Training Accuracy= 0.993\n",
            "Step 452, Minibatch Loss= 0.0349, Training Accuracy= 0.991\n",
            "Step 453, Minibatch Loss= 0.0293, Training Accuracy= 0.990\n",
            "Step 454, Minibatch Loss= 0.0263, Training Accuracy= 0.993\n",
            "Step 455, Minibatch Loss= 0.0233, Training Accuracy= 0.994\n",
            "Step 456, Minibatch Loss= 0.0233, Training Accuracy= 0.994\n",
            "Step 457, Minibatch Loss= 0.0313, Training Accuracy= 0.989\n",
            "Step 458, Minibatch Loss= 0.0240, Training Accuracy= 0.996\n",
            "Step 459, Minibatch Loss= 0.0226, Training Accuracy= 0.995\n",
            "Step 460, Minibatch Loss= 0.0296, Training Accuracy= 0.991\n",
            "Step 461, Minibatch Loss= 0.0273, Training Accuracy= 0.994\n",
            "Step 462, Minibatch Loss= 0.0281, Training Accuracy= 0.992\n",
            "Step 463, Minibatch Loss= 0.0243, Training Accuracy= 0.994\n",
            "Step 464, Minibatch Loss= 0.0249, Training Accuracy= 0.994\n",
            "Step 465, Minibatch Loss= 0.0226, Training Accuracy= 0.996\n",
            "Step 466, Minibatch Loss= 0.0210, Training Accuracy= 0.996\n",
            "Step 467, Minibatch Loss= 0.0288, Training Accuracy= 0.994\n",
            "Step 468, Minibatch Loss= 0.0224, Training Accuracy= 0.994\n",
            "Step 469, Minibatch Loss= 0.0200, Training Accuracy= 0.995\n",
            "Step 470, Minibatch Loss= 0.0239, Training Accuracy= 0.994\n",
            "Step 471, Minibatch Loss= 0.0260, Training Accuracy= 0.995\n",
            "Step 472, Minibatch Loss= 0.0194, Training Accuracy= 0.995\n",
            "Step 473, Minibatch Loss= 0.0284, Training Accuracy= 0.992\n",
            "Step 474, Minibatch Loss= 0.0225, Training Accuracy= 0.994\n",
            "Step 475, Minibatch Loss= 0.0161, Training Accuracy= 0.998\n",
            "Step 476, Minibatch Loss= 0.0191, Training Accuracy= 0.996\n",
            "Step 477, Minibatch Loss= 0.0222, Training Accuracy= 0.991\n",
            "Step 478, Minibatch Loss= 0.0303, Training Accuracy= 0.994\n",
            "Step 479, Minibatch Loss= 0.0229, Training Accuracy= 0.994\n",
            "Step 480, Minibatch Loss= 0.0217, Training Accuracy= 0.995\n",
            "Step 481, Minibatch Loss= 0.0251, Training Accuracy= 0.994\n",
            "Step 482, Minibatch Loss= 0.0290, Training Accuracy= 0.992\n",
            "Step 483, Minibatch Loss= 0.0204, Training Accuracy= 0.996\n",
            "Step 484, Minibatch Loss= 0.0254, Training Accuracy= 0.993\n",
            "Step 485, Minibatch Loss= 0.0183, Training Accuracy= 0.996\n",
            "Step 486, Minibatch Loss= 0.0175, Training Accuracy= 0.996\n",
            "Step 487, Minibatch Loss= 0.0241, Training Accuracy= 0.994\n",
            "Step 488, Minibatch Loss= 0.0194, Training Accuracy= 0.996\n",
            "Step 489, Minibatch Loss= 0.0153, Training Accuracy= 0.999\n",
            "Step 490, Minibatch Loss= 0.0189, Training Accuracy= 0.996\n",
            "Step 491, Minibatch Loss= 0.0283, Training Accuracy= 0.990\n",
            "Step 492, Minibatch Loss= 0.0203, Training Accuracy= 0.996\n",
            "Step 493, Minibatch Loss= 0.0207, Training Accuracy= 0.994\n",
            "Step 494, Minibatch Loss= 0.0256, Training Accuracy= 0.992\n",
            "Step 495, Minibatch Loss= 0.0196, Training Accuracy= 0.996\n",
            "Step 496, Minibatch Loss= 0.0211, Training Accuracy= 0.993\n",
            "Step 497, Minibatch Loss= 0.0245, Training Accuracy= 0.993\n",
            "Step 498, Minibatch Loss= 0.0179, Training Accuracy= 0.998\n",
            "Step 499, Minibatch Loss= 0.0213, Training Accuracy= 0.995\n",
            "Step 500, Minibatch Loss= 0.0207, Training Accuracy= 0.994\n",
            "Step 501, Minibatch Loss= 0.0163, Training Accuracy= 0.998\n",
            "Step 502, Minibatch Loss= 0.0220, Training Accuracy= 0.996\n",
            "Step 503, Minibatch Loss= 0.0184, Training Accuracy= 0.997\n",
            "Step 504, Minibatch Loss= 0.0164, Training Accuracy= 0.996\n",
            "Step 505, Minibatch Loss= 0.0228, Training Accuracy= 0.994\n",
            "Step 506, Minibatch Loss= 0.0209, Training Accuracy= 0.994\n",
            "Step 507, Minibatch Loss= 0.0143, Training Accuracy= 0.998\n",
            "Step 508, Minibatch Loss= 0.0237, Training Accuracy= 0.991\n",
            "Step 509, Minibatch Loss= 0.0174, Training Accuracy= 0.996\n",
            "Step 510, Minibatch Loss= 0.0216, Training Accuracy= 0.993\n",
            "Step 511, Minibatch Loss= 0.0192, Training Accuracy= 0.995\n",
            "Step 512, Minibatch Loss= 0.0234, Training Accuracy= 0.992\n",
            "Step 513, Minibatch Loss= 0.0252, Training Accuracy= 0.994\n",
            "Step 514, Minibatch Loss= 0.0185, Training Accuracy= 0.996\n",
            "Step 515, Minibatch Loss= 0.0173, Training Accuracy= 0.996\n",
            "Step 516, Minibatch Loss= 0.0306, Training Accuracy= 0.992\n",
            "Step 517, Minibatch Loss= 0.0179, Training Accuracy= 0.996\n",
            "Step 518, Minibatch Loss= 0.0145, Training Accuracy= 0.997\n",
            "Step 519, Minibatch Loss= 0.0150, Training Accuracy= 0.996\n",
            "Step 520, Minibatch Loss= 0.0173, Training Accuracy= 0.994\n",
            "Step 521, Minibatch Loss= 0.0185, Training Accuracy= 0.998\n",
            "Step 522, Minibatch Loss= 0.0148, Training Accuracy= 0.998\n",
            "Step 523, Minibatch Loss= 0.0180, Training Accuracy= 0.997\n",
            "Step 524, Minibatch Loss= 0.0146, Training Accuracy= 0.997\n",
            "Step 525, Minibatch Loss= 0.0148, Training Accuracy= 0.996\n",
            "Step 526, Minibatch Loss= 0.0181, Training Accuracy= 0.995\n",
            "Step 527, Minibatch Loss= 0.0179, Training Accuracy= 0.994\n",
            "Step 528, Minibatch Loss= 0.0191, Training Accuracy= 0.996\n",
            "Step 529, Minibatch Loss= 0.0197, Training Accuracy= 0.994\n",
            "Step 530, Minibatch Loss= 0.0210, Training Accuracy= 0.993\n",
            "Step 531, Minibatch Loss= 0.0150, Training Accuracy= 0.996\n",
            "Step 532, Minibatch Loss= 0.0174, Training Accuracy= 0.997\n",
            "Step 533, Minibatch Loss= 0.0159, Training Accuracy= 0.996\n",
            "Step 534, Minibatch Loss= 0.0123, Training Accuracy= 0.997\n",
            "Step 535, Minibatch Loss= 0.0185, Training Accuracy= 0.996\n",
            "Step 536, Minibatch Loss= 0.0154, Training Accuracy= 0.996\n",
            "Step 537, Minibatch Loss= 0.0138, Training Accuracy= 0.997\n",
            "Step 538, Minibatch Loss= 0.0155, Training Accuracy= 0.996\n",
            "Step 539, Minibatch Loss= 0.0197, Training Accuracy= 0.993\n",
            "Step 540, Minibatch Loss= 0.0185, Training Accuracy= 0.995\n",
            "Step 541, Minibatch Loss= 0.0136, Training Accuracy= 0.996\n",
            "Step 542, Minibatch Loss= 0.0191, Training Accuracy= 0.994\n",
            "Step 543, Minibatch Loss= 0.0190, Training Accuracy= 0.994\n",
            "Step 544, Minibatch Loss= 0.0232, Training Accuracy= 0.994\n",
            "Step 545, Minibatch Loss= 0.0104, Training Accuracy= 0.998\n",
            "Step 546, Minibatch Loss= 0.0173, Training Accuracy= 0.997\n",
            "Step 547, Minibatch Loss= 0.0190, Training Accuracy= 0.994\n",
            "Step 548, Minibatch Loss= 0.0156, Training Accuracy= 0.993\n",
            "Step 549, Minibatch Loss= 0.0189, Training Accuracy= 0.996\n",
            "Step 550, Minibatch Loss= 0.0283, Training Accuracy= 0.991\n",
            "Step 551, Minibatch Loss= 0.0158, Training Accuracy= 0.997\n",
            "Step 552, Minibatch Loss= 0.0152, Training Accuracy= 0.996\n",
            "Step 553, Minibatch Loss= 0.0147, Training Accuracy= 0.996\n",
            "Step 554, Minibatch Loss= 0.0181, Training Accuracy= 0.996\n",
            "Step 555, Minibatch Loss= 0.0190, Training Accuracy= 0.996\n",
            "Step 556, Minibatch Loss= 0.0123, Training Accuracy= 0.998\n",
            "Step 557, Minibatch Loss= 0.0206, Training Accuracy= 0.991\n",
            "Step 558, Minibatch Loss= 0.0198, Training Accuracy= 0.994\n",
            "Step 559, Minibatch Loss= 0.0177, Training Accuracy= 0.995\n",
            "Step 560, Minibatch Loss= 0.0174, Training Accuracy= 0.995\n",
            "Step 561, Minibatch Loss= 0.0119, Training Accuracy= 0.998\n",
            "Step 562, Minibatch Loss= 0.0188, Training Accuracy= 0.995\n",
            "Step 563, Minibatch Loss= 0.0210, Training Accuracy= 0.994\n",
            "Step 564, Minibatch Loss= 0.0114, Training Accuracy= 0.999\n",
            "Step 565, Minibatch Loss= 0.0199, Training Accuracy= 0.994\n",
            "Step 566, Minibatch Loss= 0.0209, Training Accuracy= 0.993\n",
            "Step 567, Minibatch Loss= 0.0127, Training Accuracy= 0.997\n",
            "Step 568, Minibatch Loss= 0.0105, Training Accuracy= 0.999\n",
            "Step 569, Minibatch Loss= 0.0194, Training Accuracy= 0.993\n",
            "Step 570, Minibatch Loss= 0.0183, Training Accuracy= 0.995\n",
            "Step 571, Minibatch Loss= 0.0214, Training Accuracy= 0.994\n",
            "Step 572, Minibatch Loss= 0.0116, Training Accuracy= 0.999\n",
            "Step 573, Minibatch Loss= 0.0104, Training Accuracy= 0.999\n",
            "Step 574, Minibatch Loss= 0.0185, Training Accuracy= 0.995\n",
            "Step 575, Minibatch Loss= 0.0143, Training Accuracy= 0.997\n",
            "Step 576, Minibatch Loss= 0.0136, Training Accuracy= 0.996\n",
            "Step 577, Minibatch Loss= 0.0158, Training Accuracy= 0.996\n",
            "Step 578, Minibatch Loss= 0.0116, Training Accuracy= 0.998\n",
            "Step 579, Minibatch Loss= 0.0134, Training Accuracy= 0.999\n",
            "Step 580, Minibatch Loss= 0.0125, Training Accuracy= 0.998\n",
            "Step 581, Minibatch Loss= 0.0166, Training Accuracy= 0.996\n",
            "Step 582, Minibatch Loss= 0.0182, Training Accuracy= 0.997\n",
            "Step 583, Minibatch Loss= 0.0136, Training Accuracy= 0.996\n",
            "Step 584, Minibatch Loss= 0.0119, Training Accuracy= 0.999\n",
            "Step 585, Minibatch Loss= 0.0181, Training Accuracy= 0.996\n",
            "Step 586, Minibatch Loss= 0.0222, Training Accuracy= 0.994\n",
            "Step 587, Minibatch Loss= 0.0141, Training Accuracy= 0.996\n",
            "Step 588, Minibatch Loss= 0.0115, Training Accuracy= 0.999\n",
            "Step 589, Minibatch Loss= 0.0190, Training Accuracy= 0.994\n",
            "Step 590, Minibatch Loss= 0.0190, Training Accuracy= 0.992\n",
            "Step 591, Minibatch Loss= 0.0226, Training Accuracy= 0.994\n",
            "Step 592, Minibatch Loss= 0.0135, Training Accuracy= 0.997\n",
            "Step 593, Minibatch Loss= 0.0124, Training Accuracy= 0.997\n",
            "Step 594, Minibatch Loss= 0.0098, Training Accuracy= 0.999\n",
            "Step 595, Minibatch Loss= 0.0169, Training Accuracy= 0.997\n",
            "Step 596, Minibatch Loss= 0.0200, Training Accuracy= 0.994\n",
            "Step 597, Minibatch Loss= 0.0113, Training Accuracy= 0.997\n",
            "Step 598, Minibatch Loss= 0.0176, Training Accuracy= 0.996\n",
            "Step 599, Minibatch Loss= 0.0122, Training Accuracy= 0.998\n",
            "Step 600, Minibatch Loss= 0.0155, Training Accuracy= 0.997\n",
            "Step 601, Minibatch Loss= 0.0140, Training Accuracy= 0.997\n",
            "Step 602, Minibatch Loss= 0.0125, Training Accuracy= 0.996\n",
            "Step 603, Minibatch Loss= 0.0093, Training Accuracy= 0.999\n",
            "Step 604, Minibatch Loss= 0.0125, Training Accuracy= 0.999\n",
            "Step 605, Minibatch Loss= 0.0119, Training Accuracy= 0.998\n",
            "Step 606, Minibatch Loss= 0.0131, Training Accuracy= 0.996\n",
            "Step 607, Minibatch Loss= 0.0091, Training Accuracy= 0.997\n",
            "Step 608, Minibatch Loss= 0.0126, Training Accuracy= 0.997\n",
            "Step 609, Minibatch Loss= 0.0149, Training Accuracy= 0.996\n",
            "Step 610, Minibatch Loss= 0.0125, Training Accuracy= 0.998\n",
            "Step 611, Minibatch Loss= 0.0138, Training Accuracy= 0.996\n",
            "Step 612, Minibatch Loss= 0.0105, Training Accuracy= 0.998\n",
            "Step 613, Minibatch Loss= 0.0151, Training Accuracy= 0.995\n",
            "Step 614, Minibatch Loss= 0.0119, Training Accuracy= 0.997\n",
            "Step 615, Minibatch Loss= 0.0090, Training Accuracy= 0.998\n",
            "Step 616, Minibatch Loss= 0.0149, Training Accuracy= 0.997\n",
            "Step 617, Minibatch Loss= 0.0085, Training Accuracy= 0.999\n",
            "Step 618, Minibatch Loss= 0.0080, Training Accuracy= 0.999\n",
            "Step 619, Minibatch Loss= 0.0155, Training Accuracy= 0.996\n",
            "Step 620, Minibatch Loss= 0.0132, Training Accuracy= 0.996\n",
            "Step 621, Minibatch Loss= 0.0108, Training Accuracy= 0.997\n",
            "Step 622, Minibatch Loss= 0.0087, Training Accuracy= 0.997\n",
            "Step 623, Minibatch Loss= 0.0139, Training Accuracy= 0.996\n",
            "Step 624, Minibatch Loss= 0.0134, Training Accuracy= 0.996\n",
            "Step 625, Minibatch Loss= 0.0119, Training Accuracy= 0.996\n",
            "Step 626, Minibatch Loss= 0.0179, Training Accuracy= 0.994\n",
            "Step 627, Minibatch Loss= 0.0089, Training Accuracy= 0.999\n",
            "Step 628, Minibatch Loss= 0.0091, Training Accuracy= 0.999\n",
            "Step 629, Minibatch Loss= 0.0116, Training Accuracy= 0.996\n",
            "Step 630, Minibatch Loss= 0.0130, Training Accuracy= 0.997\n",
            "Step 631, Minibatch Loss= 0.0094, Training Accuracy= 0.999\n",
            "Step 632, Minibatch Loss= 0.0075, Training Accuracy= 0.999\n",
            "Step 633, Minibatch Loss= 0.0097, Training Accuracy= 0.999\n",
            "Step 634, Minibatch Loss= 0.0152, Training Accuracy= 0.996\n",
            "Step 635, Minibatch Loss= 0.0094, Training Accuracy= 0.999\n",
            "Step 636, Minibatch Loss= 0.0119, Training Accuracy= 0.998\n",
            "Step 637, Minibatch Loss= 0.0160, Training Accuracy= 0.996\n",
            "Step 638, Minibatch Loss= 0.0127, Training Accuracy= 0.997\n",
            "Step 639, Minibatch Loss= 0.0080, Training Accuracy= 0.999\n",
            "Step 640, Minibatch Loss= 0.0115, Training Accuracy= 0.996\n",
            "Step 641, Minibatch Loss= 0.0135, Training Accuracy= 0.998\n",
            "Step 642, Minibatch Loss= 0.0157, Training Accuracy= 0.996\n",
            "Step 643, Minibatch Loss= 0.0128, Training Accuracy= 0.996\n",
            "Step 644, Minibatch Loss= 0.0148, Training Accuracy= 0.996\n",
            "Step 645, Minibatch Loss= 0.0111, Training Accuracy= 0.998\n",
            "Step 646, Minibatch Loss= 0.0090, Training Accuracy= 0.997\n",
            "Step 647, Minibatch Loss= 0.0106, Training Accuracy= 0.998\n",
            "Step 648, Minibatch Loss= 0.0112, Training Accuracy= 0.997\n",
            "Step 649, Minibatch Loss= 0.0065, Training Accuracy= 0.999\n",
            "Step 650, Minibatch Loss= 0.0083, Training Accuracy= 0.999\n",
            "Step 651, Minibatch Loss= 0.0105, Training Accuracy= 0.998\n",
            "Step 652, Minibatch Loss= 0.0165, Training Accuracy= 0.995\n",
            "Step 653, Minibatch Loss= 0.0153, Training Accuracy= 0.997\n",
            "Step 654, Minibatch Loss= 0.0105, Training Accuracy= 0.996\n",
            "Step 655, Minibatch Loss= 0.0106, Training Accuracy= 0.999\n",
            "Step 656, Minibatch Loss= 0.0135, Training Accuracy= 0.996\n",
            "Step 657, Minibatch Loss= 0.0183, Training Accuracy= 0.992\n",
            "Step 658, Minibatch Loss= 0.0105, Training Accuracy= 0.996\n",
            "Step 659, Minibatch Loss= 0.0227, Training Accuracy= 0.991\n",
            "Step 660, Minibatch Loss= 0.0119, Training Accuracy= 0.997\n",
            "Step 661, Minibatch Loss= 0.0156, Training Accuracy= 0.995\n",
            "Step 662, Minibatch Loss= 0.0166, Training Accuracy= 0.994\n",
            "Step 663, Minibatch Loss= 0.0177, Training Accuracy= 0.996\n",
            "Step 664, Minibatch Loss= 0.0214, Training Accuracy= 0.993\n",
            "Step 665, Minibatch Loss= 0.0114, Training Accuracy= 0.996\n",
            "Step 666, Minibatch Loss= 0.0144, Training Accuracy= 0.996\n",
            "Step 667, Minibatch Loss= 0.0208, Training Accuracy= 0.994\n",
            "Step 668, Minibatch Loss= 0.0137, Training Accuracy= 0.996\n",
            "Step 669, Minibatch Loss= 0.0095, Training Accuracy= 0.998\n",
            "Step 670, Minibatch Loss= 0.0127, Training Accuracy= 0.996\n",
            "Step 671, Minibatch Loss= 0.0192, Training Accuracy= 0.992\n",
            "Step 672, Minibatch Loss= 0.0158, Training Accuracy= 0.995\n",
            "Step 673, Minibatch Loss= 0.0152, Training Accuracy= 0.996\n",
            "Step 674, Minibatch Loss= 0.0205, Training Accuracy= 0.994\n",
            "Step 675, Minibatch Loss= 0.0125, Training Accuracy= 0.996\n",
            "Step 676, Minibatch Loss= 0.0129, Training Accuracy= 0.998\n",
            "Step 677, Minibatch Loss= 0.0169, Training Accuracy= 0.996\n",
            "Step 678, Minibatch Loss= 0.0110, Training Accuracy= 0.998\n",
            "Step 679, Minibatch Loss= 0.0080, Training Accuracy= 0.998\n",
            "Step 680, Minibatch Loss= 0.0126, Training Accuracy= 0.996\n",
            "Step 681, Minibatch Loss= 0.0093, Training Accuracy= 0.999\n",
            "Step 682, Minibatch Loss= 0.0131, Training Accuracy= 0.996\n",
            "Step 683, Minibatch Loss= 0.0086, Training Accuracy= 0.999\n",
            "Step 684, Minibatch Loss= 0.0099, Training Accuracy= 0.998\n",
            "Step 685, Minibatch Loss= 0.0076, Training Accuracy= 0.999\n",
            "Step 686, Minibatch Loss= 0.0079, Training Accuracy= 0.999\n",
            "Step 687, Minibatch Loss= 0.0090, Training Accuracy= 0.998\n",
            "Step 688, Minibatch Loss= 0.0114, Training Accuracy= 0.997\n",
            "Step 689, Minibatch Loss= 0.0099, Training Accuracy= 0.998\n",
            "Step 690, Minibatch Loss= 0.0092, Training Accuracy= 0.999\n",
            "Step 691, Minibatch Loss= 0.0087, Training Accuracy= 0.999\n",
            "Step 692, Minibatch Loss= 0.0074, Training Accuracy= 0.998\n",
            "Step 693, Minibatch Loss= 0.0090, Training Accuracy= 0.998\n",
            "Step 694, Minibatch Loss= 0.0077, Training Accuracy= 0.998\n",
            "Step 695, Minibatch Loss= 0.0065, Training Accuracy= 0.999\n",
            "Step 696, Minibatch Loss= 0.0093, Training Accuracy= 0.998\n",
            "Step 697, Minibatch Loss= 0.0087, Training Accuracy= 0.999\n",
            "Step 698, Minibatch Loss= 0.0179, Training Accuracy= 0.996\n",
            "Step 699, Minibatch Loss= 0.0093, Training Accuracy= 0.998\n",
            "Step 700, Minibatch Loss= 0.0123, Training Accuracy= 0.999\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:101: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.93143815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-MLRt-1aTZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('done saving at',save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvzl_SXTdXK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"TF_ModelwBN_for_Paper4.ckpt.meta\")\n",
        "files.download(\"TF_ModelwBN_for_Paper4.ckpt.index\")\n",
        "files.download(\"TF_ModelwBN_for_Paper4.ckpt.data-00000-of-00001\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}